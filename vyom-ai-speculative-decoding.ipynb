{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\nfrom torch.nn import Module\nfrom typing import List, Tuple, Union\nfrom torch import Tensor\nfrom transformers.cache_utils import DynamicCache, StaticCache","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T13:13:25.129069Z","iopub.execute_input":"2025-07-06T13:13:25.129621Z","iopub.status.idle":"2025-07-06T13:13:33.873985Z","shell.execute_reply.started":"2025-07-06T13:13:25.129597Z","shell.execute_reply":"2025-07-06T13:13:33.873401Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"checkpoint = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n\ndevice = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\nmodel.eval()\nmessages = [{\"role\": \"user\", \"content\": \"What is gravity?\"}]\ninput_text=tokenizer.apply_chat_template(messages, tokenize=False)\ninputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs, max_new_tokens=50, temperature=0.2, top_p=0.9, do_sample=True)\nprint(tokenizer.decode(outputs[0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T13:13:33.875157Z","iopub.execute_input":"2025-07-06T13:13:33.875585Z","iopub.status.idle":"2025-07-06T13:13:53.511920Z","shell.execute_reply.started":"2025-07-06T13:13:33.875566Z","shell.execute_reply":"2025-07-06T13:13:53.510964Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f799057af4c4dd7ab67322918227cdb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4a2d121d42e4c91a0091d355fedb29e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f69ff570443a4523aa2bd5bff451ae4f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2de5e06c83534202ac7505da1d824db3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/655 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c9295d8325742fbbce9f53c4fda7270"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/861 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1498d771a3f44061a2b96ea6e1982ead"}},"metadata":{}},{"name":"stderr","text":"2025-07-06 13:13:37.248761: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1751807617.430897      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1751807617.484344      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/269M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3b400c1b9d049b9b50a9aff1bc9d4f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f037803b19a14a278558f22aa352313f"}},"metadata":{}},{"name":"stderr","text":"The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"name":"stdout","text":"<|im_start|>system\nYou are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\n<|im_start|>user\nWhat is gravity?<|im_end|>\n<|im_start|>assistant\nGravity is a fundamental force of nature that attracts objects with mass towards each other. It is a result of the interaction between mass, energy, and space itself. According to Einstein's theory of general relativity, gravity is not a\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"checkpoint = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n\nmodel1 = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\nmodel1.eval()\nmessages = [{\"role\": \"user\", \"content\":  \"What is gravity?\"}]\ninput_text=tokenizer.apply_chat_template(messages, tokenize=False)\ninputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\noutputs = model1.generate(inputs, max_new_tokens=50, temperature=0.2, top_p=0.9, do_sample=True)\nprint(tokenizer.decode(outputs[0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T13:13:53.512860Z","iopub.execute_input":"2025-07-06T13:13:53.513477Z","iopub.status.idle":"2025-07-06T13:13:59.690587Z","shell.execute_reply.started":"2025-07-06T13:13:53.513450Z","shell.execute_reply":"2025-07-06T13:13:59.689898Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/846 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b974031997b44cff8f6b78617723af7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/724M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b553d60cf9540ac8f37d967158ef057"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d254094b43734497befbaed5c3573007"}},"metadata":{}},{"name":"stdout","text":"<|im_start|>system\nYou are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\n<|im_start|>user\nWhat is gravity?<|im_end|>\n<|im_start|>assistant\nGravity is a fundamental force of nature that causes objects with mass to attract each other. It is a universal force that acts between two objects with mass, regardless of their distance from each other. According to Einstein's theory of general\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"messages = [{\"role\": \"user\", \"content\":  \"What is the capital of France.\"}]\ninput_text=tokenizer.apply_chat_template(messages, tokenize=False)\ninputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\noutputs = model1.generate(inputs, max_new_tokens=50, temperature=0.2, top_p=0.9, do_sample=True)\nprint(tokenizer.decode(outputs[0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T13:13:59.692366Z","iopub.execute_input":"2025-07-06T13:13:59.692561Z","iopub.status.idle":"2025-07-06T13:14:00.074813Z","shell.execute_reply.started":"2025-07-06T13:13:59.692546Z","shell.execute_reply":"2025-07-06T13:14:00.074157Z"}},"outputs":[{"name":"stdout","text":"<|im_start|>system\nYou are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\n<|im_start|>user\nWhat is the capital of France.<|im_end|>\n<|im_start|>assistant\nThe capital of France is Paris.<|im_end|>\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"messages = [{\"role\": \"user\", \"content\":  \"What is the capital of France.\"}]\ninput_text=tokenizer.apply_chat_template(messages, tokenize=False)\ninputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\noutputs = model1.generate(inputs, max_new_tokens=50,assistant_model=model, temperature=0.2, top_p=0.9, do_sample=True)\nprint(tokenizer.decode(outputs[0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T13:14:00.075518Z","iopub.execute_input":"2025-07-06T13:14:00.075817Z","iopub.status.idle":"2025-07-06T13:14:00.546369Z","shell.execute_reply.started":"2025-07-06T13:14:00.075792Z","shell.execute_reply":"2025-07-06T13:14:00.545546Z"}},"outputs":[{"name":"stdout","text":"<|im_start|>system\nYou are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\n<|im_start|>user\nWhat is the capital of France.<|im_end|>\n<|im_start|>assistant\nThe capital of France is Paris.<|im_end|>\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"messages = [{\"role\": \"user\", \"content\":  \"Once upon a time,\"}]\ninput_text=tokenizer.apply_chat_template(messages, tokenize=False)\ninputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\noutputs = model1.generate(inputs, max_new_tokens=50,assistant_model=model, temperature=0.2, top_p=0.9, do_sample=True)\nprint(tokenizer.decode(outputs[0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T13:14:00.547242Z","iopub.execute_input":"2025-07-06T13:14:00.547522Z","iopub.status.idle":"2025-07-06T13:14:02.337812Z","shell.execute_reply.started":"2025-07-06T13:14:00.547498Z","shell.execute_reply":"2025-07-06T13:14:02.337169Z"}},"outputs":[{"name":"stdout","text":"<|im_start|>system\nYou are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\n<|im_start|>user\nOnce upon a time,<|im_end|>\n<|im_start|>assistant\nOnce upon a time, there was a young girl named Lily who lived in a small village surrounded by rolling hills and dense forests. She loved to play in the meadows and forests, and spent her days exploring the woods and meadows.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"messages = [{\"role\": \"user\", \"content\":  \"Once upon a time,\"}]\ninput_text=tokenizer.apply_chat_template(messages, tokenize=False)\ninputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\noutputs = model1.generate(inputs, max_new_tokens=50,temperature=0.2, top_p=0.9, do_sample=True)\nprint(tokenizer.decode(outputs[0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T13:14:02.338467Z","iopub.execute_input":"2025-07-06T13:14:02.338675Z","iopub.status.idle":"2025-07-06T13:14:03.963019Z","shell.execute_reply.started":"2025-07-06T13:14:02.338638Z","shell.execute_reply":"2025-07-06T13:14:03.962176Z"}},"outputs":[{"name":"stdout","text":"<|im_start|>system\nYou are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\n<|im_start|>user\nOnce upon a time,<|im_end|>\n<|im_start|>assistant\nOnce upon a time, there was a young girl named Lily who lived in a small village surrounded by rolling hills and dense forests. She had a kind heart and a strong sense of justice, which made her a great leader for her\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import time\ntimes = []\nfor _ in range(3):\n    start = time.time()\n    outputs = model1.generate(inputs, max_new_tokens=128,temperature=0.2, top_p=0.9, do_sample=True, use_cache=True)\n    end = time.time()\n    times.append(end - start)\n\n# Report average\navg_time = sum(times) / len(times)\nprint(f\"Average generation time per prompt: {avg_time:.4f} seconds\")\nprint(times)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T13:14:03.963981Z","iopub.execute_input":"2025-07-06T13:14:03.964259Z","iopub.status.idle":"2025-07-06T13:14:16.684986Z","shell.execute_reply.started":"2025-07-06T13:14:03.964231Z","shell.execute_reply":"2025-07-06T13:14:16.684412Z"}},"outputs":[{"name":"stdout","text":"Average generation time per prompt: 4.2385 seconds\n[4.472963094711304, 4.098721742630005, 4.143867492675781]\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import time\ntimes = []\nfor _ in range(3):\n    start = time.time()\n    outputs = model1.generate(inputs, max_new_tokens=128,assistant_model=model, temperature=0.2, top_p=0.9, do_sample=True,use_cache=True)\n    end = time.time()\n    times.append(end - start)\n\n# Report average\navg_time = sum(times) / len(times)\nprint(f\"Average generation time per prompt: {avg_time:.4f} seconds\")\nprint(times)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T13:14:16.685638Z","iopub.execute_input":"2025-07-06T13:14:16.685889Z","iopub.status.idle":"2025-07-06T13:14:39.548482Z","shell.execute_reply.started":"2025-07-06T13:14:16.685872Z","shell.execute_reply":"2025-07-06T13:14:39.547892Z"}},"outputs":[{"name":"stdout","text":"Average generation time per prompt: 7.6194 seconds\n[6.938439130783081, 9.12900972366333, 6.790746450424194]\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"def prune_cache(cache: Union[Tuple[Tuple[Tensor, Tensor]], DynamicCache], num_tokens_to_discard: int):\n    \"\"\"\n    Prune the cache by removing the specified number of tokens from the end.\n\n    Args:\n        cache (Union[Tuple[Tuple[Tensor, Tensor]], DynamicCache]): The KV cache to be pruned.\n        num_tokens_to_discard (int): The number of tokens to discard from the end of the cache.\n\n    Returns:\n        Union[Tuple[Tuple[Tensor, Tensor]], DynamicCache]: The pruned KV cache.\n    \"\"\"\n    if cache is None:\n        return None\n        \n    if isinstance(cache, DynamicCache):\n        return prune_dynamic_cache(cache, num_tokens_to_discard)\n    elif isinstance(cache, StaticCache):\n        return prune_static_cache(cache, num_tokens_to_discard)\n    else:\n        raise ValueError(\"Unsupported cache type.\")\n\n\n\n\ndef prune_dynamic_cache(cache: DynamicCache, num_tokens_to_discard: int):\n    \"\"\"\n    Prune the cache by removing the specified number of tokens from the end. This pruning works for models using DynamicCache.\n\n    Args:\n        cache (DynamicCache): The KV cache to be pruned.\n        num_tokens_to_discard (int): The number of tokens to discard from the end of the cache.\n\n    Returns:\n        DynamicCache: The pruned KV cache. (same instance as the input cache, but modified in place)\n    \"\"\"\n    if cache is None:\n        return None\n\n    for layer in range(len(cache)):\n        cache.key_cache[layer] = cache.key_cache[layer][:, :, :-num_tokens_to_discard, :]\n        cache.value_cache[layer] = cache.value_cache[layer][:, :, :-num_tokens_to_discard, :]\n    cache._seen_tokens -= num_tokens_to_discard\n\n    return cache\n\ndef prune_static_cache(cache: DynamicCache, num_tokens_to_discard: int):\n    \"\"\"\n    Prune the cache by removing the specified number of tokens from the end. This pruning works for models using DynamicCache.\n\n    Args:\n        cache (DynamicCache): The KV cache to be pruned.\n        num_tokens_to_discard (int): The number of tokens to discard from the end of the cache.\n\n    Returns:\n        DynamicCache: The pruned KV cache. (same instance as the input cache, but modified in place)\n    \"\"\"\n    if cache is None:\n        return None\n\n    for layer in range(len(cache)):\n        cache.key_cache[layer][:, :, num_tokens_to_discard-1:, :] = 0 \n        cache.value_cache[layer][:, :, num_tokens_to_discard-1:, :] = 0\n    cache._seen_tokens -= num_tokens_to_discard\n\n    return cache","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T13:16:01.933339Z","iopub.execute_input":"2025-07-06T13:16:01.933942Z","iopub.status.idle":"2025-07-06T13:16:01.942435Z","shell.execute_reply.started":"2025-07-06T13:16:01.933913Z","shell.execute_reply":"2025-07-06T13:16:01.941840Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"import abc\nimport torch\nfrom torch import Tensor\nfrom torch.nn import functional as F\n\nclass GreedyProcessor():\n    \"\"\"Greedy: Most probable token.\"\"\"\n\n    def __init__(self, temperature: float = 1):\n        self.temperature = temperature\n\n    def __call__(self, logits: Tensor) -> Tensor:\n        return F.softmax(logits/self.temperature, dim=-1)\n\n    def sample(self, probs: Tensor) -> Tensor:\n        return torch.argmax(probs, dim=-1).unsqueeze(-1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T13:14:39.558951Z","iopub.execute_input":"2025-07-06T13:14:39.559216Z","iopub.status.idle":"2025-07-06T13:14:39.579023Z","shell.execute_reply.started":"2025-07-06T13:14:39.559200Z","shell.execute_reply":"2025-07-06T13:14:39.578330Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def max_fn(x: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Max function.\n        x: input tensor.\n    Returns:\n        tensor norm(max(0, x)).\n    \"\"\"\n    x_max = torch.where(x > 0, x, torch.zeros_like(x))\n    x_max_sum = torch.sum(x_max, dim=-1, keepdim=True)\n    return x_max / x_max_sum\n\n\n@torch.no_grad()\ndef speculative_generate(\n    inputs: List[int],\n    drafter: Module,\n    target: Module,\n    tokenizer = None,\n    gamma: int = 5,\n    logits_processor = GreedyProcessor(),\n    max_gen_len: int = 128,\n    eos_tokens_id: int | List[int] = 2,\n    pad_token_id: int = 2,\n    use_cache: bool = False,\n    skip_sample_adjustment: bool = False,\n    first_target: bool = True,\n    use_dynamic_cache: bool = False\n\n) -> Tuple[List[int], float]:\n    \"\"\"\n    Generate text sequence using the speculative decoding algorithm.\n    Implementation of Speculative Decoding. (https://arxiv.org/pdf/2211.17192.pdf)\n    \n    Args:\n        inputs (List[int]): input sequence of batch size 1.\n        drafter (Module): drafter model.\n        target (Module): target model.\n        tokenizer: tokenizer (used for debugging).\n        gamma (int): number of drafts generated by the drafter at each step.\n        logits_processor (LogitsProcessor): logits processor for sampling.\n        max_gen_len (int): maximum length of the generated sequence.\n        eos_tokens_id (int or List[int]): end token id (could be multiple).\n        pad_token_id (int): pad token id.\n        use_cache (bool): whether to use cache.\n        skip_sample_adjustment (bool): whether to skip the sample adjustment step when some drafts are discarded.\n        first_target (bool): whether to run the target model before the speculative algorithm.\n         use_dynamic_cache (bool) : use StaticCache or DynamicCache\n    \n    Returns:\n        List[int]: generated sequence.\n        float: acceptance rate (number of accepted drafts divided by the number of total drafts).\n        \n    Note: This generation methods only works for decoder-only models.\n    Note bis: The drafter and target models should output the same logits shape.\n    \"\"\"\n    #only works for batch size of 1\n    if use_dynamic_cache:\n        drafter_cache, target_cache = DynamicCache(),DynamicCache()\n    else:\n         drafter_cache, target_cache = StaticCache(drafter.config,1,device=drafter.device), StaticCache(target.config,1,device=drafter.device) #\n\n    stop_tokens_id = eos_tokens_id if isinstance(eos_tokens_id, list) else [eos_tokens_id]\n    stop_tokens = torch.tensor(stop_tokens_id, dtype=torch.long, device=target.device).unsqueeze(1)\n    \n    drafts_accepted, drafts_speculated = .0, .0\n    \n    vocabulary_size = target.config.vocab_size    \n        \n    # prepare input tensor\n    prompt_len = len(inputs[0])\n    max_seq_length = target.config.max_position_embeddings if hasattr(target.config, 'max_position_embeddings') else (target.config.max_context_length if hasattr(target.config, 'max_context_length') else 1024)\n    total_len = min(max_seq_length, prompt_len + max_gen_len)\n    input_ids = torch.full((1, total_len), pad_token_id, dtype=torch.long, device=target.device)\n    input_ids[0, :prompt_len] = inputs \n    \n    current_position = prompt_len\n    \n    if first_target:\n        # run the target model before the speculative algorithm. Allows to prefill the kvcache and get a first token.\n        target_output = target(\n            input_ids=input_ids[..., :current_position],\n            past_key_values=target_cache,\n            use_cache=use_cache,\n        )\n        target_cache = target_output.past_key_values\n        p_p = logits_processor(target_output.logits[..., -1, :])\n        t = logits_processor.sample(p_p)\n        input_ids[0, current_position] = t\n        current_position += 1\n        \n        if torch.isin(t, stop_tokens):\n            return input_ids[0, prompt_len:current_position].tolist(), 0\n    \n    while current_position < total_len:\n        corrected_gamma = min(gamma, total_len - current_position - 1)\n        q = torch.zeros((1, corrected_gamma, vocabulary_size), device=target.device)\n        \n        input_ids = input_ids.to(drafter.device)\n        \n        # generate gamma drafts\n        for k in range(corrected_gamma):\n            draft_output = drafter(\n                input_ids=input_ids[..., :current_position + k],\n                past_key_values=drafter_cache,\n                use_cache=use_cache,\n            )\n            drafter_cache = draft_output.past_key_values\n            \n            draft_logits = draft_output.logits[..., -1, :]\n            draft_probs = logits_processor(draft_logits)\n            q[0, k] = draft_probs.to(target.device)\n            xi = logits_processor.sample(draft_probs)\n            input_ids[0, current_position + k] = xi\n        drafts_speculated += corrected_gamma\n        input_ids = input_ids.to(target.device)\n        \n        # run target model on drafts and get logits of the previous tokens plus one more token\n        target_output = target(\n            input_ids=input_ids[..., :current_position + corrected_gamma],\n            past_key_values=target_cache,\n            use_cache=use_cache,\n        )\n        target_cache = target_output.past_key_values\n        draft_logits = target_output.logits[..., current_position - 1:current_position + corrected_gamma - 1, :] # [1, corrected_gamma, vocab_size]\n        p = logits_processor(draft_logits) # [1, gamma, vocab_size]\n        \n        # compute the last accepted draft position (rejection sampling)\n        r = torch.rand(corrected_gamma, device=target.device)\n        fractions = p / q\n        n = corrected_gamma\n        for i in range(corrected_gamma):\n            if r[i] > fractions[0, i, input_ids[0, current_position + i]]:\n                n = i\n                break\n        \n        drafts_accepted += n\n        \n        # check if the end token is in the drafts\n        stop_locations = torch.nonzero(torch.eq(input_ids[..., current_position:current_position + n], stop_tokens))\n        if stop_locations.shape[0] > 0:\n            stop_location = stop_locations[0, 1].item()\n            return input_ids[0, prompt_len:current_position + stop_location + 1].tolist(), drafts_accepted / drafts_speculated\n\n        # adjust the distribution from target_output\n        if n == corrected_gamma:\n            p_p = target_output.logits[..., current_position + corrected_gamma - 1, :]\n            p_p = logits_processor(p_p)\n        else:\n            # prune the cache\n            if use_cache:\n                drafter_cache = prune_cache(drafter_cache, corrected_gamma - n)\n                target_cache = prune_cache(target_cache, corrected_gamma - n + 1)\n            \n            if not skip_sample_adjustment:\n                p_p = max_fn(p[..., n, :] - q[0, n, :])\n            else:\n                p_p = p[..., n, :]\n        x = logits_processor.sample(p_p)\n            \n        input_ids[0, current_position + n:current_position + corrected_gamma] = pad_token_id\n        input_ids[0, current_position + n] = x\n            \n        current_position += n + 1\n        \n        if torch.isin(x, stop_tokens):\n            return input_ids[0, prompt_len:current_position].tolist(), drafts_accepted / drafts_speculated\n    \n    return input_ids[0, prompt_len:].tolist(), drafts_accepted / drafts_speculated\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T13:14:39.579867Z","iopub.execute_input":"2025-07-06T13:14:39.580452Z","iopub.status.idle":"2025-07-06T13:14:39.597746Z","shell.execute_reply.started":"2025-07-06T13:14:39.580428Z","shell.execute_reply":"2025-07-06T13:14:39.597162Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"messages = [{\"role\": \"user\", \"content\":  \"What is the capital of France.\"}]\ninput_text=tokenizer.apply_chat_template(messages, tokenize=False)\ninputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n\nout = speculative_generate(inputs,model, model1,tokenizer)\nprint(tokenizer.decode(out[0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T13:14:39.598492Z","iopub.execute_input":"2025-07-06T13:14:39.598792Z","iopub.status.idle":"2025-07-06T13:14:42.103992Z","shell.execute_reply.started":"2025-07-06T13:14:39.598771Z","shell.execute_reply":"2025-07-06T13:14:42.103200Z"}},"outputs":[{"name":"stdout","text":"<|im_start|>assistant\nI'm sorry, but as an AI, I don't have the ability to access real-time information or provide geographical information. I'm designed to provide information based on the knowledge base and user input. I recommend using a reliable travel app or a map service to find the capital of France.<|im_end|>\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"out = speculative_generate(inputs,model, model1,tokenizer,use_dynamic_cache=True)\nprint(tokenizer.decode(out[0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T13:14:42.104831Z","iopub.execute_input":"2025-07-06T13:14:42.105503Z","iopub.status.idle":"2025-07-06T13:14:44.733312Z","shell.execute_reply.started":"2025-07-06T13:14:42.105483Z","shell.execute_reply":"2025-07-06T13:14:44.732521Z"}},"outputs":[{"name":"stdout","text":"<|im_start|>assistant\nI'm sorry, but as an AI trained on Hugging Face's dataset, I don't have access to real-world knowledge about geographical locations or national capitals. I recommend using a reliable online encyclopedia or a map application to find the correct information.<|im_end|>\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"messages = [{\"role\": \"user\", \"content\":  \"Once upon a time,\"}]\ninput_text=tokenizer.apply_chat_template(messages, tokenize=False)\ninputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n\nout = speculative_generate(inputs,model, model1,tokenizer)\nprint(tokenizer.decode(out[0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T13:16:37.328528Z","iopub.execute_input":"2025-07-06T13:16:37.329217Z","iopub.status.idle":"2025-07-06T13:16:43.651321Z","shell.execute_reply.started":"2025-07-06T13:16:37.329194Z","shell.execute_reply":"2025-07-06T13:16:43.650705Z"}},"outputs":[{"name":"stdout","text":"<|im_start|>assistant\nOnce upon a time, there was a young girl named Lily who lived in a small village surrounded by rolling hills and dense forests. She loved playing with her pet rabbit, Benny, and spending time in the village green. One day, while playing with Benny, Lily noticed that the trees seemed to be changing colors. The leaves were turning a beautiful shade of orange, red, and yellow.\n\nIntrigued, Lily decided to ask her grandmother, Mrs. Jenkins, who lived in the next village over, about this phenomenon. She asked Mrs. Jenkins to tell her about the changing colors of the leaves, but Mrs.\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"out = speculative_generate(inputs,model, model1,tokenizer,use_dynamic_cache=True)\nprint(tokenizer.decode(out[0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T13:14:49.847109Z","iopub.execute_input":"2025-07-06T13:14:49.847354Z","iopub.status.idle":"2025-07-06T13:14:54.890443Z","shell.execute_reply.started":"2025-07-06T13:14:49.847338Z","shell.execute_reply":"2025-07-06T13:14:54.889845Z"}},"outputs":[{"name":"stdout","text":"<|im_start|>assistant\nOnce upon a time, there was a young girl named Lily who lived in a small village surrounded by rolling hills and dense forests. She loved spending her days playing in the meadows and forests, exploring the woods, and listening to the stories of the old people. One day, while wandering through the forest, she stumbled upon a hidden clearing. In the center of the clearing stood a magnificent tree with a trunk as wide as the village itself, its branches stretching towards the sky like a giant's arm.\n\nAs Lily gazed at the tree, she noticed a small, delicate flower that bloomed in the center of\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\ntimes = []\nfor _ in range(3):\n    start = time.time()\n    outputs = speculative_generate(inputs,model, model1,tokenizer)\n    end = time.time()\n    times.append(end - start)\n\n# Report average\navg_time = sum(times) / len(times)\nprint(f\"Average generation time per prompt: {avg_time:.4f} seconds\")\nprint(times)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T13:14:54.891206Z","iopub.execute_input":"2025-07-06T13:14:54.891458Z","iopub.status.idle":"2025-07-06T13:15:11.503283Z","shell.execute_reply.started":"2025-07-06T13:14:54.891437Z","shell.execute_reply":"2025-07-06T13:15:11.502517Z"}},"outputs":[{"name":"stdout","text":"Average generation time per prompt: 5.5357 seconds\n[5.54840350151062, 6.19432258605957, 4.864379644393921]\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}