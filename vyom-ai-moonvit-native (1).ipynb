{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1111749,"sourceType":"datasetVersion","datasetId":623329}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"p_config = {\n    \"auto_map\": {\n        \"AutoImageProcessor\": \"image_processing_moonvit.MoonViTImageProcessor\"\n    },\n    \"in_token_limit\": 4096,\n    \"patch_size\": 14,\n    \"num_pooled_tokens\": 1024,\n    \"image_mean\": [\n        0.5,\n        0.5,\n        0.5\n    ],\n    \"image_std\": [\n        0.5,\n        0.5,\n        0.5\n    ],\n    \"pad_input\": True\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T08:25:37.534085Z","iopub.execute_input":"2025-07-20T08:25:37.534373Z","iopub.status.idle":"2025-07-20T08:25:37.541132Z","shell.execute_reply.started":"2025-07-20T08:25:37.534350Z","shell.execute_reply":"2025-07-20T08:25:37.540381Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers.configuration_utils import PretrainedConfig\n\n\nclass MoonViTConfig(PretrainedConfig):\n    model_type = \"moonvit\"\n\n    def __init__(\n        self,\n        patch_size: int = 16,\n        init_pos_emb_height: int = 64,\n        init_pos_emb_width: int = 64,\n        num_attention_heads: int = 12,\n        num_hidden_layers: int = 8,\n        hidden_size: int = 768,\n        intermediate_size: int = 3072,\n        merge_kernel_size: tuple[int, int] = (2, 2),\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.patch_size = patch_size\n        # Positional embedding config\n        self.init_pos_emb_height = init_pos_emb_height\n        self.init_pos_emb_width = init_pos_emb_width\n        # Transformer config\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n        self.hidden_size = hidden_size\n        self.intermediate_size = intermediate_size\n        # Patch merger config\n        self.merge_kernel_size = merge_kernel_size\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T08:25:37.544632Z","iopub.execute_input":"2025-07-20T08:25:37.544837Z","iopub.status.idle":"2025-07-20T08:25:44.609775Z","shell.execute_reply.started":"2025-07-20T08:25:37.544821Z","shell.execute_reply":"2025-07-20T08:25:44.609215Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import math\nimport numpy as np\nfrom PIL import Image\nfrom typing import Optional, Union\n\nimport torch\nfrom torchvision.transforms import functional as TF\nfrom transformers.image_utils import ImageInput, make_list_of_images, valid_images\nfrom transformers.image_processing_utils import BaseImageProcessor, BatchFeature\nfrom transformers.utils import TensorType\n\n\nOPENAI_DATASET_MEAN = (0.48145466, 0.4578275, 0.40821073)\nOPENAI_DATASET_STD = (0.26862954, 0.26130258, 0.27577711)\n\n\nclass MoonViTImageProcessor(BaseImageProcessor):\n    model_type = \"moonvit\"\n\n    def __init__(\n        self,\n        image_mean: tuple[float, float, float] = OPENAI_DATASET_MEAN,\n        image_std: tuple[float, float, float] = OPENAI_DATASET_STD,\n        in_token_limit: int = 4096,\n        patch_size: int = 16,\n        pad_input: bool = True,\n        merge_kernel_size: list[int, int] = [2, 2],\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.in_token_limit = in_token_limit\n        self.patch_size = patch_size\n        self.pad_input = pad_input\n        self.image_mean = image_mean\n        self.image_std = image_std\n        self.merge_kernel_size = merge_kernel_size\n\n    def rescale(\n        self, image: Image.Image, merge_kernel_size: list[int, int] = [2, 2]\n    ) -> Image.Image:\n        w, h = image.size\n        patch_size = self.patch_size\n\n        if (w // patch_size) * (h // patch_size) > self.in_token_limit:\n            scale = math.sqrt(self.in_token_limit / ((w // patch_size) * (h // patch_size)))\n            new_w, new_h = int(w * scale), int(h * scale)\n            image = image.resize((new_w, new_h), Image.Resampling.BICUBIC)\n        if self.pad_input:\n            new_w, new_h = image.size\n            pad_size_h = merge_kernel_size[0] * patch_size\n            pad_size_w = merge_kernel_size[1] * patch_size\n\n            pad_h = (pad_size_h - new_h % pad_size_h) % pad_size_h\n            pad_w = (pad_size_w - new_w % pad_size_w) % pad_size_w\n\n            image = TF.pad(image, (0, 0, pad_w, pad_h))\n        else:\n            new_w, new_h = image.size\n            new_w = new_w - new_w % patch_size\n            new_h = new_h - new_h % patch_size\n            image = TF.center_crop(image, (new_h, new_w))\n\n        w, h = image.size\n        if w // patch_size >= 512 or h // patch_size >= 512:\n            raise ValueError(\"Exceed pos emb\")\n\n        return image\n\n    def to_tensor(self, image: Image.Image) -> torch.Tensor:\n        return TF.to_tensor(image.convert(\"RGB\"))\n\n    def normalize(self, image: torch.Tensor) -> torch.Tensor:\n        return TF.normalize(image, self.image_mean, self.image_std)\n\n    def patchify(self, image: torch.Tensor) -> tuple[torch.Tensor, list[int, int]]:\n        patch_size = self.patch_size\n        C, H, W = image.shape\n        patches = image.reshape(C, H // patch_size, patch_size, W // patch_size, patch_size)\n        patches = patches.permute(1, 3, 0, 2, 4)\n        patches = patches.contiguous().view(-1, C, patch_size, patch_size)\n        grid_hw = (H // patch_size, W // patch_size)\n        return patches, grid_hw\n\n    def _preprocess(self, image: ImageInput) -> tuple[torch.Tensor, list[int, int]]:\n        \"\"\"\n        Preprocess image and patchify it.\n        Args:\n            image (`ImageInput`):\n                Image to preprocess. Expects pixel values ranging from 0 to 255. If pixel values range from 0 to 1, set `do_rescale=False`.\n        Returns:\n            patches: torch.Tensor\n            grid_hw: list[int, int]\n        \"\"\"\n        image = self.rescale(image, self.merge_kernel_size)\n        image = self.to_tensor(image)\n        image = self.normalize(image)\n        patches, grid_hw = self.patchify(image)\n        return patches, grid_hw\n\n    def preprocess(\n        self,\n        images: ImageInput,\n        return_tensors: Optional[Union[str, TensorType]] = None,\n    ) -> BatchFeature:\n        images = make_list_of_images(images)\n\n        if not valid_images(images):\n            raise ValueError(\n                \"Invalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \"\n                \"torch.Tensor, tf.Tensor or jax.ndarray.\"\n            )\n\n        pixel_values, image_grid_hws = [], []\n        for image in images:\n            patches, image_grid_hw = self._preprocess(image)\n            pixel_values.append(patches)\n            image_grid_hws.append(image_grid_hw)\n        pixel_values = torch.concat(pixel_values, dim=0)\n        image_grid_hws = np.array(image_grid_hws)\n        data = {\"pixel_values\": pixel_values, \"image_grid_hws\": image_grid_hws}\n\n        return BatchFeature(data=data, tensor_type=return_tensors)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T08:25:44.610928Z","iopub.execute_input":"2025-07-20T08:25:44.611310Z","iopub.status.idle":"2025-07-20T08:26:03.115134Z","shell.execute_reply.started":"2025-07-20T08:25:44.611290Z","shell.execute_reply":"2025-07-20T08:26:03.114621Z"}},"outputs":[{"name":"stderr","text":"2025-07-20 08:25:50.120084: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1752999950.350649      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1752999950.420045      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"img_processor = MoonViTImageProcessor()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T08:26:03.115781Z","iopub.execute_input":"2025-07-20T08:26:03.116251Z","iopub.status.idle":"2025-07-20T08:26:03.121218Z","shell.execute_reply.started":"2025-07-20T08:26:03.116230Z","shell.execute_reply":"2025-07-20T08:26:03.119797Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from torchvision import transforms\nto_pil = transforms.ToPILImage()\nimages = [to_pil(torch.rand(3, 256, 384)),to_pil(torch.rand(3, 256, 256))]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T08:26:03.123267Z","iopub.execute_input":"2025-07-20T08:26:03.123619Z","iopub.status.idle":"2025-07-20T08:26:03.193723Z","shell.execute_reply.started":"2025-07-20T08:26:03.123592Z","shell.execute_reply":"2025-07-20T08:26:03.192978Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"images_processed = img_processor(images, return_tensors=\"pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T08:26:03.194376Z","iopub.execute_input":"2025-07-20T08:26:03.194571Z","iopub.status.idle":"2025-07-20T08:26:03.221667Z","shell.execute_reply.started":"2025-07-20T08:26:03.194557Z","shell.execute_reply":"2025-07-20T08:26:03.220989Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"images_processed['pixel_values'].size()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T08:26:03.222414Z","iopub.execute_input":"2025-07-20T08:26:03.222660Z","iopub.status.idle":"2025-07-20T08:26:03.228018Z","shell.execute_reply.started":"2025-07-20T08:26:03.222639Z","shell.execute_reply":"2025-07-20T08:26:03.227475Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"torch.Size([640, 3, 16, 16])"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"images_processed.image_grid_hws","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T08:26:03.228675Z","iopub.execute_input":"2025-07-20T08:26:03.229048Z","iopub.status.idle":"2025-07-20T08:26:03.245446Z","shell.execute_reply.started":"2025-07-20T08:26:03.229030Z","shell.execute_reply":"2025-07-20T08:26:03.244950Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"tensor([[16, 24],\n        [16, 16]])"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"import math\nfrom copy import deepcopy\nfrom typing import Union, Tuple, Sequence, Optional, List\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers.activations import PytorchGELUTanh\nfrom transformers.modeling_utils import PreTrainedModel\nfrom transformers.utils import is_flash_attn_2_available\n\n\n\n# if is_flash_attn_2_available():\n#     from flash_attn import flash_attn_varlen_func\n# else:\n#     flash_attn_varlen_func = None\n\n\ndef multihead_attention(\n    q: torch.Tensor,\n    k: torch.Tensor,\n    v: torch.Tensor,\n    q_cu_seqlens: Optional[torch.Tensor] = None,\n    k_cu_seqlens: Optional[torch.Tensor] = None,\n):\n    \"\"\"Multi-head attention using flash attention 2.\n    Args:\n        q, k, v: tensor of shape (batch_size, seqlen, num_heads, head_dim),\n            or (tot_seqlens, num_heads, head_dim) if packing.\n        q_cu_seqlens (torch.Tensor): cumulative sequence lengths of q.\n            The first element should be 0 and the last element should be q.shape[0].\n        k_cu_seqlens (torch.Tensor): cumulative sequence lengths of k.\n            The first element should be 0 and the last element should be k.shape[0].\n    Returns:\n        output: shape (batch_size, seqlen, dim) or (tot_seqlens, dim) if packing,\n            where dim = num_heads * head_dim\n    \"\"\"\n    # Unified format legal check\n    assert q.dim() == k.dim() == v.dim() == 3, \"q, k, v must have 3 dims\"\n    assert q_cu_seqlens[-1] == q.shape[0], \"q_cu_seqlens must sum to q.shape[0]\"\n    assert (\n        k_cu_seqlens[-1] == k.shape[0] == v.shape[0]\n    ), \"k_cu_seqlens must sum to k.shape[0]\"\n    assert q.dtype in [\n        torch.bfloat16,\n        torch.float16,\n    ], f\"unsupported dtype {q.dtype} for multihead attn\"\n\n    max_seqlen_q = (q_cu_seqlens[1:] - q_cu_seqlens[:-1]).max().item()\n    max_seqlen_k = (k_cu_seqlens[1:] - k_cu_seqlens[:-1]).max().item()\n    attn_out = flash_attn_varlen_func(\n        q,\n        k,\n        v,\n        q_cu_seqlens,\n        k_cu_seqlens,\n        max_seqlen_q,\n        max_seqlen_k,\n        causal=False,\n    )\n    attn_out = attn_out.flatten(start_dim=-2)\n\n    return attn_out\n\n\ndef sdpa_attention(\n    q: torch.Tensor,\n    k: torch.Tensor,\n    v: torch.Tensor,\n    q_cu_seqlens: Optional[torch.Tensor] = None,\n    k_cu_seqlens: Optional[torch.Tensor] = None,\n) -> torch.Tensor:\n    \"\"\"SDPA attention.\n    Args:\n        q, k, v: tensor of shape (batch_size, seqlen, num_heads, head_dim),\n            or (tot_seqlens, num_heads, head_dim) if packing.\n    \"\"\"\n    seq_length = q.shape[0]\n    attention_mask = torch.zeros(\n        [1, seq_length, seq_length], device=q.device, dtype=torch.bool\n    )\n    for i in range(1, len(q_cu_seqlens)):\n        attention_mask[\n            ...,\n            q_cu_seqlens[i - 1] : q_cu_seqlens[i],\n            q_cu_seqlens[i - 1] : q_cu_seqlens[i],\n        ] = True\n    q = q.transpose(0, 1)\n    k = k.transpose(0, 1)\n    v = v.transpose(0, 1)\n    attn_output = F.scaled_dot_product_attention(q, k, v, attention_mask, dropout_p=0.0)\n    attn_output = attn_output.transpose(0, 1)\n    attn_output = attn_output.reshape(seq_length, -1)\n    return attn_output\n\n\ndef eager_attention(\n    q: torch.Tensor,\n    k: torch.Tensor,\n    v: torch.Tensor,\n    q_cu_seqlens: Optional[torch.Tensor] = None,\n    k_cu_seqlens: Optional[torch.Tensor] = None,\n) -> torch.Tensor:\n    seq_length = q.shape[0]\n    attention_mask = torch.zeros(\n        [1, seq_length, seq_length], device=q.device, dtype=torch.bool\n    )\n    for i in range(1, len(q_cu_seqlens)):\n        attention_mask[\n            ...,\n            q_cu_seqlens[i - 1] : q_cu_seqlens[i],\n            q_cu_seqlens[i - 1] : q_cu_seqlens[i],\n        ] = True\n    q = q.transpose(0, 1)\n    k = k.transpose(0, 1)\n    v = v.transpose(0, 1)\n\n    attn_weight = q @ k.transpose(-2, -1) / math.sqrt(q.shape[-1])\n    attn_weight += attention_mask\n    attn_weight = torch.softmax(attn_weight, dim=-1, dtype=torch.float32).to(q.dtype)\n\n    attn_output = attn_weight @ v\n    attn_output = attn_output.transpose(0, 1)\n    attn_output = attn_output.reshape(seq_length, -1)\n    return attn_output\n\n\nVL_VISION_ATTENTION_FUNCTIONS = {\n    # \"flash_attention_2\": multihead_attention,\n    \"sdpa\": sdpa_attention,\n    \"eager\": eager_attention,\n}\n\n\ndef _apply_rope_input_validation(x, freqs_cis):\n    assert x.ndim == freqs_cis.ndim + 1, (x.shape, freqs_cis.shape)\n    assert x.shape[:-2] == freqs_cis.shape[:-1], (x.shape, freqs_cis.shape)\n    assert x.shape[-1] == 2 * freqs_cis.shape[-1], (x.shape, freqs_cis.shape)\n    assert freqs_cis.dtype == torch.complex64, freqs_cis.dtype\n\ndef rotate_half(x):\n    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n    x1 = x[..., : x.shape[-1] // 2]\n    x2 = x[..., x.shape[-1] // 2 :]\n    return torch.cat((-x2, x1), dim=-1)\n\n\ndef apply_rotary_pos_emb_vision(tensor: torch.Tensor, freqs: torch.Tensor) -> torch.Tensor:\n    orig_dtype = tensor.dtype\n    tensor = tensor.float()\n    cos = freqs.cos()\n    sin = freqs.sin()\n    cos = cos.unsqueeze(1).repeat(1, 1, 2).unsqueeze(0).float()\n    sin = sin.unsqueeze(1).repeat(1, 1, 2).unsqueeze(0).float()\n    output = (tensor * cos) + (rotate_half(tensor) * sin)\n    output = output.to(orig_dtype)\n    return output\n    \ndef apply_rope(\n    xq: torch.Tensor, xk: torch.Tensor, freqs_cis: torch.Tensor\n) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Args: (The leading dimensions of all inputs should be the same)\n        xq: query, tensor of shape (..., num_heads, head_dim)\n        xk: key, tensor of shape (..., num_heads, head_dim)\n        freqs_cis: tensor of shape (..., head_dim/2), dtype=torch.complex64. It contains the precomputed cis(freqs) for each position in the 2D grid.\n    Returns:\n        xq_out, xk_out: tensors of shape (..., num_heads, head_dim)\n    \"\"\"\n    \n    _apply_rope_input_validation(xq, freqs_cis)\n    _apply_rope_input_validation(xk, freqs_cis)\n\n    freqs_cis = freqs_cis.unsqueeze(-2)  # ..., 1, head_dim/2\n    # ..., num_heads, head_dim/2\n    xq_ = torch.view_as_complex(xq.float().view(*xq.shape[:-1], -1, 2))\n    xk_ = torch.view_as_complex(xk.float().view(*xq.shape[:-1], -1, 2))\n    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(-2)  # ..., num_heads, head_dim\n    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(-2)  # ..., num_heads, head_dim\n    return xq_out.type_as(xq), xk_out.type_as(xk)\n\ndef apply_rope_real(\n    xq: torch.Tensor, xk: torch.Tensor, freqs: torch.Tensor\n) -> tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Args:\n        xq, xk: (..., num_heads, head_dim)\n        freqs: (..., head_dim//2) in radians\n    Returns:\n        xq_out, xk_out: (..., num_heads, head_dim)\n    \"\"\"\n    def rotate_half(x):\n        x1 = x[..., : x.shape[-1] // 2]\n        x2 = x[..., x.shape[-1] // 2 :]\n        return torch.cat((-x2, x1), dim=-1)\n    \n    orig_dtype = xq.dtype\n    xq = xq.float()\n    xk = xk.float()\n\n    # Convert freqs (radians) into cos and sin\n    cos = freqs.cos().unsqueeze(-2)  # match head dimension\n    sin = freqs.sin().unsqueeze(-2)\n\n    # Expand to match xq/xk shape\n    cos = cos.repeat_interleave(2, dim=-1)  # from head_dim/2 to head_dim\n    sin = sin.repeat_interleave(2, dim=-1)\n\n    xq_out = (xq * cos) + (rotate_half(xq) * sin)\n    xk_out = (xk * cos) + (rotate_half(xk) * sin)\n\n    return xq_out.to(orig_dtype), xk_out.to(orig_dtype)\n\nclass Learnable2DInterpPosEmb(nn.Module):\n    def __init__(\n        self, height: int, width: int, dim: int, interpolation_mode: str = \"bicubic\"\n    ) -> None:\n        super().__init__()\n        self.height = height\n        self.width = width\n        self.interpolation_mode = interpolation_mode\n        self.weight = nn.Parameter(torch.empty(height, width, dim))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.normal_(self.weight)\n\n    def forward(self, x: torch.Tensor, grid_hws: torch.Tensor) -> torch.Tensor:\n        pos_embs = []\n        for shape in grid_hws.tolist():\n            if shape == self.weight.shape[:-1]:\n                pos_embs.append(self.weight.flatten(end_dim=1))\n            else:\n                pos_embs.append(\n                    F.interpolate(\n                        self.weight.permute((2, 0, 1)).unsqueeze(0),\n                        size=shape,\n                        mode=self.interpolation_mode,\n                    )\n                    .squeeze(0)\n                    .permute((1, 2, 0))\n                    .flatten(end_dim=1)\n                )\n        out = x + torch.cat(pos_embs)\n        return out\n\n\nclass MoonVisionPatchEmbed(nn.Module):\n\n    def __init__(\n        self,\n        out_dim: int,\n        in_dim: int = 3,\n        patch_size: Union[int, Tuple[int, int]] = (16, 16),\n        pos_emb_height: int = 16,\n        pos_emb_width: int = 16,\n    ):\n        super().__init__()\n        assert isinstance(\n            patch_size, (int, Sequence)\n        ), f\"Invalid patch_size type: {type(patch_size)}\"\n        if isinstance(patch_size, int):\n            patch_size = (patch_size, patch_size)\n        assert (\n            len(patch_size) == 2\n        ), f\"Expected patch_size to be a tuple of 2, got {patch_size}\"\n        self.patch_size = patch_size\n\n        self.proj = nn.Conv2d(\n            in_dim, out_dim, kernel_size=patch_size, stride=patch_size\n        )\n\n        self.pos_emb = Learnable2DInterpPosEmb(\n            height=pos_emb_height, width=pos_emb_width, dim=out_dim\n        )\n\n    def forward(self, x: torch.Tensor, grid_hws: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            x (L, Channels): input tensor\n            grid_hws (N, 2): grid height and width\n        Returns:\n            (L, Cout) tensor\n        \"\"\"\n        x = self.proj(x).view(x.size(0), -1)\n        # apply positional embedding\n        x = self.pos_emb(x, grid_hws)\n        return x\n\n\nclass Rope2DPosEmb(nn.Module):\n    \"\"\"2D rotary position embedding with multi-resolution support.\n    This class is intended to be used in the following way:\n    1. Before training, create an instance of Rope2DPosEmb. This instance will hold the precomputed cis.\n    2. Before each forward pass, call `get_freqs_cis_by_*` to get the `freqs_cis` tensor for this iteration.\n    3. During the forward pass, pass the `freqs_cis` tensor to each attention layer, and call `apply` just before each attention operation.\n        The rope is shared across all attention layers and all heads.\n    Refs:\n    - RoFormer: https://arxiv.org/abs/2104.09864\n    - VisionLLaMA: https://arxiv.org/abs/2403.00522\n    - https://github.com/Meituan-AutoML/VisionLLaMA/blob/main/dit/models.py\n    Args:\n        dim (int): usually the multi-head attention dimension, should be divisible by 4 (TODO: relax this constraint if needed)\n        max_height (int): the maximum height of the 2D grid\n        max_width (int): the maximum width of the 2D grid\n        theta_base (float): the base of the theta\n        device (str): the device to store the precomputed cis\n    \"\"\"\n\n    def __init__(self, dim: int, max_height: int, max_width: int, theta_base=10000):\n        super().__init__()\n        self.dim = dim\n        assert self.dim % 4 == 0, \"dim must be divisible by 4\"\n        self.max_height = max_height\n        self.max_width = max_width\n        self.theta_base = theta_base\n\n        self.freqs_cis = None\n\n    def extra_repr(self):\n        return f\"dim={self.dim}, max_height={self.max_height}, max_width={self.max_width}, theta_base={self.theta_base}\"\n\n    def _precompute_freqs_cis(self, device: torch.device) -> torch.Tensor:\n        \"\"\"Calculate the cis(freqs) for each position in the 2D grid.\n        Return: complex tensor of shape (max_height, max_width, dim//2) and value:\n            height axis: ret[h, w, 2*i] = cis(h * theta_base**(-4*i/dim))\n            weight axis: ret[h, w, 2*i+1] = cis(w * theta_base**(-4*i/dim))   with (i in [0, dim//4))\n            note: `cis` is a mathematical notation defined by cis x = cos x + i sin x,\n        \"\"\"\n        N = self.max_height * self.max_width\n        flat_pos = torch.arange(0, N).float().to(device)\n        x_pos = flat_pos % self.max_width\n        y_pos = flat_pos // self.max_width\n        dim_range = (\n            torch.arange(0, self.dim, 4)[: (self.dim // 4)].float().to(device)\n        )  # C/4\n        freqs = 1.0 / (self.theta_base ** (dim_range / self.dim))\n        x_freqs = torch.outer(x_pos, freqs).float()  # N, C/4\n        y_freqs = torch.outer(y_pos, freqs).float()  # N, C/4\n        x_cis = torch.polar(torch.ones_like(x_freqs), x_freqs)  # N, C/4\n        y_cis = torch.polar(torch.ones_like(y_freqs), y_freqs)  # N, C/4\n        # N, C/4, 2\n        freqs_cis = torch.cat(\n            [x_cis.unsqueeze(dim=-1), y_cis.unsqueeze(dim=-1)], dim=-1\n        )\n        # max_height, max_width, C/2\n        freqs_cis = freqs_cis.reshape(self.max_height, self.max_width, -1)\n        return freqs_cis\n\n    def get_freqs_cis(self, grid_hws: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            grid_hws (torch.Tensor): grid height and width\n        Returns:\n            freqs_cis: tensor of shape (sum(t * height * width), dim//2)\n        \"\"\"\n        if self.freqs_cis is None:\n            self.freqs_cis = self._precompute_freqs_cis(grid_hws.device)\n\n        shapes = grid_hws.tolist()\n        assert all(\n            1 <= h <= self.max_height and 1 <= w <= self.max_width for h, w in shapes\n        ), (\n            shapes,\n            self.max_height,\n            self.max_width,\n        )\n        freqs_cis = torch.cat(\n            [self.freqs_cis[:h, :w].reshape(-1, self.dim // 2) for h, w in shapes],\n            dim=0,\n        )\n        return freqs_cis\n\n\nclass MLP2(nn.Module):\n    \"\"\"\n    Args:\n        dims: [in_dim, hidden_dim, out_dim]\n        bias: whether to use bias in linear layer.\n    \"\"\"\n\n    def __init__(self, dims: list[int], activation, bias=True):\n        super().__init__()\n        assert len(dims) == 3\n        self.fc0 = nn.Linear(dims[0], dims[1], bias=bias)\n        self.fc1 = nn.Linear(dims[1], dims[2], bias=bias)\n        self.activation = activation\n        for m in [self.fc0, self.fc1]:\n            nn.init.trunc_normal_(m.weight, std=math.sqrt(2 / m.in_features))\n            if m.bias is not None:\n                nn.init.zeros_(m.bias)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.fc0(x)\n        x = self.activation(x)\n        return self.fc1(x)\n\n\nclass MoonVitEncoderLayer(nn.Module):\n\n    def __init__(\n        self,\n        num_heads: int,\n        hidden_dim: int,\n        mlp_dim: int,\n        *,\n        attn_implementation: str = \"eager\",\n        activation=F.gelu,\n        attn_bias: bool = False,\n    ):\n        super().__init__()\n        self.num_heads = num_heads\n        self.hidden_dim = hidden_dim\n        self.hidden_size_per_attention_head = self.hidden_dim // self.num_heads\n        self.attn_implementation = attn_implementation\n\n        self.norm0 = nn.LayerNorm(hidden_dim)\n        self.norm1 = nn.LayerNorm(hidden_dim)\n        self.mlp = MLP2([hidden_dim, mlp_dim, hidden_dim], activation)\n        self.wqkv = nn.Linear(hidden_dim, hidden_dim * 3, bias=attn_bias)\n        self.wo = nn.Linear(hidden_dim, hidden_dim, bias=attn_bias)\n\n    def attention_qkvpacked(\n        self,\n        x: torch.Tensor,\n        cu_seqlens: torch.Tensor,\n        rope_freqs_cis: Optional[torch.Tensor] = None,\n    ):\n        \"\"\"\n        Args:\n            x (torch.Tensor): (batch_size, seqlen, hidden_dim)\n            cu_seqlens (torch.Tensor):\n        \"\"\"\n        xqkv = self.wqkv(x)\n\n        qkv_shape = xqkv.size()[:-1] + (\n            3,\n            self.num_heads,\n            self.hidden_size_per_attention_head,\n        )\n        # xqkv: (batch_size, seqlen, 3, nheads, headdim)\n        xqkv = xqkv.view(*qkv_shape)\n        xq, xk, xv = torch.unbind(xqkv, dim=-3)\n       \n        xq, xk = apply_rope_real(xq, xk, rope_freqs_cis)\n\n        attn_func = VL_VISION_ATTENTION_FUNCTIONS[self.attn_implementation]\n        attn_out = attn_func(\n            xq, xk, xv, q_cu_seqlens=cu_seqlens, k_cu_seqlens=cu_seqlens\n        )\n\n        attn_out = self.wo(attn_out)\n        return attn_out\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        cu_seqlens: torch.Tensor,\n        rope_freqs_cis: Union[torch.Tensor, None] = None,\n    ) -> torch.Tensor:\n        \"\"\"\n        Args:\n            hidden_states: non-packed (B, N, D) or packed (L, D). if non-packed, seqlens should be None, if packed, seqlens should be set\n        Returns:\n            output: same shape of input, non-packed (B, N, D) for non-packed input, (L, D) for packed input\n        \"\"\"\n        residual = hidden_states\n        hidden_states = self.norm0(hidden_states)\n        attn_out = self.attention_qkvpacked(\n            hidden_states, cu_seqlens, rope_freqs_cis=rope_freqs_cis\n        )\n        hidden_states = residual + attn_out\n\n        residual = hidden_states\n        hidden_states = self.mlp(self.norm1(hidden_states))\n        hidden_states = residual + hidden_states\n        return hidden_states\n\n\nclass MoonVitEncoder(nn.Module):\n\n    def __init__(\n        self,\n        hidden_dim: int,\n        num_layers: int,\n        block_cfg: dict,\n    ) -> None:\n        super().__init__()\n\n        self.rope_2d = Rope2DPosEmb(\n            block_cfg[\"hidden_dim\"] // block_cfg[\"num_heads\"], 512, 512\n        )\n        self.blocks = nn.ModuleList(\n            [MoonVitEncoderLayer(**block_cfg) for _ in range(num_layers)]\n        )\n        self.final_layernorm = nn.LayerNorm(hidden_dim)\n\n    def forward(\n        self, hidden_states: torch.Tensor, grid_hws: torch.Tensor\n    ) -> torch.Tensor:\n        rope_freqs_cis = self.rope_2d.get_freqs_cis(grid_hws=grid_hws)\n\n        lengths = torch.cat(\n            (\n                torch.zeros(1, device=hidden_states.device, dtype=grid_hws.dtype),\n                grid_hws[:, 0] * grid_hws[:, 1],\n            )\n        )\n        cu_seqlens = lengths.cumsum(dim=0, dtype=torch.int32)\n\n        for _, block in enumerate(self.blocks):\n            hidden_states = block(\n                hidden_states, cu_seqlens, rope_freqs_cis=rope_freqs_cis\n            )\n\n        hidden_states = self.final_layernorm(hidden_states)\n\n        return hidden_states\n\n\ndef patch_merger(\n    x: torch.Tensor,\n    grid_hws: torch.Tensor,\n    merge_kernel_size: list[int, int] = (2, 2),\n) -> List[torch.Tensor]:\n    d_model = x.size(-1)\n\n    outputs = []\n    pre_sum = 0\n    for x_shape in grid_hws.tolist():\n        height, width = x_shape[0], x_shape[1]\n        # Get the current sequence\n        seq = x[pre_sum : pre_sum + height * width]\n        # Reshape along self.merge_kernel_size and concat to the last dimension\n        kernel_height, kernel_width = merge_kernel_size\n        new_height, new_width = height // kernel_height, width // kernel_width\n        reshaped_seq = seq.view(\n            new_height, kernel_height, new_width, kernel_width, d_model\n        )\n        reshaped_seq = reshaped_seq.permute(0, 2, 1, 3, 4).contiguous()\n        padded_seq = reshaped_seq.view(\n            new_height * new_width, kernel_height * kernel_width, -1\n        )\n        outputs.append(padded_seq)\n        pre_sum += height * width\n\n    return outputs\n\n\nclass MoonVitPretrainedModel(PreTrainedModel):\n    config_class = MoonViTConfig\n    model_type = \"moonvit\"\n    _no_split_modules = [\"PackingTransformer\"]\n    _supports_flash_attn_2 = False\n    _supports_sdpa = True\n\n    def __init__(self, config: MoonViTConfig, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n        config = deepcopy(config)\n        self.merge_kernel_size = config.merge_kernel_size\n        self.patch_size = config.patch_size\n        self.patch_embed = MoonVisionPatchEmbed(\n            out_dim=config.hidden_size,\n            patch_size=config.patch_size,\n            pos_emb_height=config.init_pos_emb_height,\n            pos_emb_width=config.init_pos_emb_width,\n        )\n\n        self.encoder = MoonVitEncoder(\n            hidden_dim=config.hidden_size,\n            num_layers=config.num_hidden_layers,\n            block_cfg={\n                \"num_heads\": config.num_attention_heads,\n                \"hidden_dim\": config.hidden_size,\n                \"mlp_dim\": config.intermediate_size,\n                \"activation\": PytorchGELUTanh(),\n                \"attn_bias\": True,\n                \"attn_implementation\": config._attn_implementation,\n            },\n        )\n\n    def forward(\n        self, pixel_values: torch.Tensor, grid_hws: torch.Tensor\n    ) -> torch.Tensor:\n        \"\"\"\n        Args:\n            pixel_values (torch.Tensor): The input pixel values.\n            grid_hws (torch.Tensor): The grid height and width.\n        Returns:\n            torch.Tensor: The output tokens.\n        \"\"\"\n        hidden_states = self.patch_embed(pixel_values, grid_hws)\n        hidden_states = self.encoder(hidden_states, grid_hws)\n        hidden_states = patch_merger(\n            hidden_states, grid_hws, merge_kernel_size=self.merge_kernel_size\n        )\n        return hidden_states","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T08:26:03.246153Z","iopub.execute_input":"2025-07-20T08:26:03.246333Z","iopub.status.idle":"2025-07-20T08:26:05.521313Z","shell.execute_reply.started":"2025-07-20T08:26:03.246320Z","shell.execute_reply":"2025-07-20T08:26:05.520493Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"model = MoonVitPretrainedModel(MoonViTConfig())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T08:26:05.522204Z","iopub.execute_input":"2025-07-20T08:26:05.522473Z","iopub.status.idle":"2025-07-20T08:26:06.335789Z","shell.execute_reply.started":"2025-07-20T08:26:05.522449Z","shell.execute_reply":"2025-07-20T08:26:06.334994Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"images = [to_pil(torch.rand(3, 256, 384)),to_pil(torch.rand(3, 256, 256))]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T08:26:06.338383Z","iopub.execute_input":"2025-07-20T08:26:06.338614Z","iopub.status.idle":"2025-07-20T08:26:06.350269Z","shell.execute_reply.started":"2025-07-20T08:26:06.338597Z","shell.execute_reply":"2025-07-20T08:26:06.349521Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"pix, hw = img_processor._preprocess(images[0])\nprint(pix.shape, hw)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T08:26:06.351051Z","iopub.execute_input":"2025-07-20T08:26:06.351737Z","iopub.status.idle":"2025-07-20T08:26:06.357144Z","shell.execute_reply.started":"2025-07-20T08:26:06.351716Z","shell.execute_reply":"2025-07-20T08:26:06.356457Z"}},"outputs":[{"name":"stdout","text":"torch.Size([384, 3, 16, 16]) (16, 24)\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"pix, hw = img_processor._preprocess(images[1])\nprint(pix.shape, hw)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T08:26:06.357841Z","iopub.execute_input":"2025-07-20T08:26:06.358150Z","iopub.status.idle":"2025-07-20T08:26:06.372358Z","shell.execute_reply.started":"2025-07-20T08:26:06.358127Z","shell.execute_reply":"2025-07-20T08:26:06.371760Z"}},"outputs":[{"name":"stdout","text":"torch.Size([256, 3, 16, 16]) (16, 16)\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"hw","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T08:26:06.372940Z","iopub.execute_input":"2025-07-20T08:26:06.373128Z","iopub.status.idle":"2025-07-20T08:26:06.385680Z","shell.execute_reply.started":"2025-07-20T08:26:06.373097Z","shell.execute_reply":"2025-07-20T08:26:06.384927Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"(16, 16)"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"images_processed = img_processor(images, return_tensors=\"pt\")\nimage_features: list = model(images_processed.pixel_values, images_processed.image_grid_hws)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T08:26:06.386415Z","iopub.execute_input":"2025-07-20T08:26:06.386646Z","iopub.status.idle":"2025-07-20T08:26:07.558079Z","shell.execute_reply.started":"2025-07-20T08:26:06.386631Z","shell.execute_reply":"2025-07-20T08:26:07.557486Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/2070201531.py:213: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at /pytorch/aten/src/ATen/native/Copy.cpp:308.)\n  return xq_out.to(orig_dtype), xk_out.to(orig_dtype)\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"image_features[0].size()\n#(16, 24)# 384//4 = 96","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T08:26:07.558778Z","iopub.execute_input":"2025-07-20T08:26:07.558988Z","iopub.status.idle":"2025-07-20T08:26:07.564146Z","shell.execute_reply.started":"2025-07-20T08:26:07.558967Z","shell.execute_reply":"2025-07-20T08:26:07.563587Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"torch.Size([96, 4, 768])"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"image_features[1].size()\n#16x16 = 256//4 = 64","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T08:26:07.564815Z","iopub.execute_input":"2025-07-20T08:26:07.565048Z","iopub.status.idle":"2025-07-20T08:26:07.582293Z","shell.execute_reply.started":"2025-07-20T08:26:07.565027Z","shell.execute_reply":"2025-07-20T08:26:07.581747Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"torch.Size([64, 4, 768])"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"from transformers.tokenization_utils_base import AddedToken\nfrom transformers import AutoTokenizer\nmodel_ckpt = \"microsoft/deberta-v3-base\"\n\ntokenizer = AutoTokenizer.from_pretrained(\n    model_ckpt, padding_side=\"left\", truncation_side=\"right\"\n)\nIMAGE_TOKEN = \"<image>\"\n\n\nimage_token = AddedToken(IMAGE_TOKEN, normalized=False, special=True)\n\n\ntokens_to_add = {\"additional_special_tokens\": [image_token]}\n\n\ntokenizer.add_special_tokens(tokens_to_add)\n\n\nimage_token_id = tokenizer.convert_tokens_to_ids(IMAGE_TOKEN)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T08:26:07.582901Z","iopub.execute_input":"2025-07-20T08:26:07.583150Z","iopub.status.idle":"2025-07-20T08:26:11.804160Z","shell.execute_reply.started":"2025-07-20T08:26:07.583130Z","shell.execute_reply":"2025-07-20T08:26:11.803561Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65196f933f4f44bbb02ac1a710143e6f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6750eb56d9e84d41b143ff833de46046"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f6dcd1b92884f63b7bc40bc454bd924"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"!ls ../input","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T08:26:11.804864Z","iopub.execute_input":"2025-07-20T08:26:11.805115Z","iopub.status.idle":"2025-07-20T08:26:11.973080Z","shell.execute_reply.started":"2025-07-20T08:26:11.805078Z","shell.execute_reply":"2025-07-20T08:26:11.972429Z"}},"outputs":[{"name":"stdout","text":"flickr30k\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv(\"../input/flickr30k/captions.txt\")\ndf = df.dropna()\ndf[\"image\"] = \"../input/flickr30k/Images/\" + df[\"image\"]\ndf.head()\ndf.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T08:26:11.974193Z","iopub.execute_input":"2025-07-20T08:26:11.974888Z","iopub.status.idle":"2025-07-20T08:26:12.549868Z","shell.execute_reply.started":"2025-07-20T08:26:11.974863Z","shell.execute_reply":"2025-07-20T08:26:12.549167Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"(158914, 2)"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"import random\nimport math\nfrom torch.utils.data import Dataset, random_split, DataLoader\nfrom PIL import Image\nimport torchvision.transforms as T\n\ndef build_string_from_input(\n    prompt,\n    image_seq_len,\n    image_token=IMAGE_TOKEN\n):\n\n\n    return f\"{image_token * image_seq_len}{prompt}\"\n\n\n\ndef get_model_inputs(input_string, suffix,max_length, tokenizer=tokenizer,default=False):\n\n\n    return_token_type_ids = False\n\n\n    if suffix:\n        suffix = suffix\n\n\n        return_token_type_ids = True\n    \n    if default==False:\n    \n        text = tokenizer.encode(suffix,add_special_tokens=False)\n        prompt = tokenizer.encode(input_string,add_special_tokens=False)\n        \n        full_text = prompt+text\n        \n        mask = torch.ones(max_length,dtype=torch.int64)\n        \n        token_type_ids = torch.zeros(max_length,dtype=torch.int64)\n        token_type_ids[-len(text):] = 1\n        \n        if len(full_text)>max_length:\n            full_text = full_text[:max_length]\n            token_type_ids = token_type_ids[:max_length]\n            mask = mask[:max_length]\n            \n           \n        else:      #left padding\n            left_pad = max_length-len(full_text)\n            full_text = [0]*left_pad+full_text\n            mask[:left_pad] = 0\n            # token_type_ids\n    \n        inputs = {'input_ids':torch.tensor(full_text),'attention_mask':mask,'token_type_ids':token_type_ids}\n    else:\n\n\n        return_token_type_ids = True if suffix is not None else False\n    \n    \n        inputs = tokenizer(\n            input_string,\n            text_pair=suffix,\n            return_token_type_ids=return_token_type_ids,\n            padding=\"max_length\",\n            add_special_tokens=False,\n            max_length=max_length,\n            truncation=True,\n            return_tensors=\"pt\",\n        )\n    return inputs\ndtype = torch.bfloat16\n\nclass ImgDataset(Dataset):\n    def __init__(self, df,  tokenizer,img_ops,default= True, transform=None):\n        self.df = df\n        self.transform = transform\n        self.img_ops = img_ops\n       \n        self.tokenizer = tokenizer\n       \n        self.type = default\n\n    def __len__(\n        self,\n    ):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        caption = self.df.caption.iloc[idx] + tokenizer.eos_token\n        img_path = self.df.image.iloc[idx]\n        img = Image.open(img_path).convert(\"RGB\")\n\n        input_data, grid_shape = self.img_ops._preprocess(img)\n        l = math.prod(grid_shape)\n        prompt = tokenizer.bos_token +\" \"+ \"Explain this image.\"\n        input_string = build_string_from_input(prompt,image_seq_len=l)\n        \n        return input_data, grid_shape, l, input_string, caption","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T08:26:12.550719Z","iopub.execute_input":"2025-07-20T08:26:12.551004Z","iopub.status.idle":"2025-07-20T08:26:12.560792Z","shell.execute_reply.started":"2025-07-20T08:26:12.550986Z","shell.execute_reply":"2025-07-20T08:26:12.560015Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"myd = ImgDataset(df,tokenizer,img_processor)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T08:26:12.561538Z","iopub.execute_input":"2025-07-20T08:26:12.561802Z","iopub.status.idle":"2025-07-20T08:26:12.586541Z","shell.execute_reply.started":"2025-07-20T08:26:12.561775Z","shell.execute_reply":"2025-07-20T08:26:12.585869Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"from transformers.image_processing_utils import BaseImageProcessor, BatchFeature\ndef custom_collate_fn(batch):\n    input_data, grid_shape, img_prompt_size, img_prompt, caption = zip(*batch) # Unpack the tuples\n    max_img_prompt_size = max(img_prompt_size)\n    text_out =  [] \n    ids, mask,tids = [], [], []\n    for prom, cap in zip(img_prompt,caption):\n        out = get_model_inputs(prom,cap,max_length=max_img_prompt_size+64)\n        ids.append(out['input_ids'].unsqueeze(0))\n        mask.append(out['attention_mask'].unsqueeze(0))\n        tids.append(out['token_type_ids'].unsqueeze(0))\n\n    text_out = {'input_ids':torch.cat(ids),'attention_mask':torch.cat(mask),'token_type_ids':torch.cat(tids)}\n    pixel_values = torch.concat(input_data, dim=0)\n    image_grid_hws = np.array(grid_shape)\n    data = {\"pixel_values\": pixel_values, \"image_grid_hws\": image_grid_hws}\n    return {'img_info':BatchFeature(data=data,tensor_type='pt'), \"text_info\":text_out}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T08:26:12.587244Z","iopub.execute_input":"2025-07-20T08:26:12.587419Z","iopub.status.idle":"2025-07-20T08:26:12.603298Z","shell.execute_reply.started":"2025-07-20T08:26:12.587405Z","shell.execute_reply":"2025-07-20T08:26:12.602588Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"batch_size = 4\ndataloader = DataLoader(ImgDataset(df,tokenizer,img_processor), batch_size=batch_size, collate_fn=custom_collate_fn)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T08:26:12.604016Z","iopub.execute_input":"2025-07-20T08:26:12.604819Z","iopub.status.idle":"2025-07-20T08:26:12.618937Z","shell.execute_reply.started":"2025-07-20T08:26:12.604795Z","shell.execute_reply":"2025-07-20T08:26:12.618422Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"for d in dataloader:\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T08:26:12.619613Z","iopub.execute_input":"2025-07-20T08:26:12.619873Z","iopub.status.idle":"2025-07-20T08:26:12.753771Z","shell.execute_reply.started":"2025-07-20T08:26:12.619856Z","shell.execute_reply":"2025-07-20T08:26:12.753146Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"d['img_info']['pixel_values'].size()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T08:26:12.754722Z","iopub.execute_input":"2025-07-20T08:26:12.754958Z","iopub.status.idle":"2025-07-20T08:26:12.759548Z","shell.execute_reply.started":"2025-07-20T08:26:12.754936Z","shell.execute_reply":"2025-07-20T08:26:12.758889Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"torch.Size([2816, 3, 16, 16])"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"device = torch.device(\"cuda\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T08:26:12.760230Z","iopub.execute_input":"2025-07-20T08:26:12.760438Z","iopub.status.idle":"2025-07-20T08:26:12.779140Z","shell.execute_reply.started":"2025-07-20T08:26:12.760418Z","shell.execute_reply":"2025-07-20T08:26:12.778464Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"model = MoonVitPretrainedModel(MoonViTConfig())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T08:26:12.779807Z","iopub.execute_input":"2025-07-20T08:26:12.780042Z","iopub.status.idle":"2025-07-20T08:26:13.567977Z","shell.execute_reply.started":"2025-07-20T08:26:12.780023Z","shell.execute_reply":"2025-07-20T08:26:13.567404Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"model.to(torch.bfloat16).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T08:26:13.571520Z","iopub.execute_input":"2025-07-20T08:26:13.571741Z","iopub.status.idle":"2025-07-20T08:26:13.773626Z","shell.execute_reply.started":"2025-07-20T08:26:13.571725Z","shell.execute_reply":"2025-07-20T08:26:13.773023Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"MoonVitPretrainedModel(\n  (patch_embed): MoonVisionPatchEmbed(\n    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n    (pos_emb): Learnable2DInterpPosEmb()\n  )\n  (encoder): MoonVitEncoder(\n    (rope_2d): Rope2DPosEmb(dim=64, max_height=512, max_width=512, theta_base=10000)\n    (blocks): ModuleList(\n      (0-7): 8 x MoonVitEncoderLayer(\n        (norm0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): MLP2(\n          (fc0): Linear(in_features=768, out_features=3072, bias=True)\n          (fc1): Linear(in_features=3072, out_features=768, bias=True)\n          (activation): PytorchGELUTanh()\n        )\n        (wqkv): Linear(in_features=768, out_features=2304, bias=True)\n        (wo): Linear(in_features=768, out_features=768, bias=True)\n      )\n    )\n    (final_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n)"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"from tqdm.notebook import tqdm\nfor i, d in tqdm(enumerate(dataloader)):\n    out = model(d['img_info']['pixel_values'].to(torch.bfloat16).to(device),d['img_info']['image_grid_hws'].to(device))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T08:26:13.774367Z","iopub.execute_input":"2025-07-20T08:26:13.774627Z","iopub.status.idle":"2025-07-20T08:30:31.955404Z","shell.execute_reply.started":"2025-07-20T08:26:13.774605Z","shell.execute_reply":"2025-07-20T08:30:31.954305Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1275f5f8b3fa4d18a712b430216539c9"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/2678704716.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotebook\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'img_info'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pixel_values'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'img_info'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_grid_hws'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/2070201531.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values, grid_hws)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \"\"\"\n\u001b[1;32m    593\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid_hws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid_hws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m         hidden_states = patch_merger(\n\u001b[1;32m    596\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid_hws\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmerge_kernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_kernel_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/2070201531.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, grid_hws)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 513\u001b[0;31m             hidden_states = block(\n\u001b[0m\u001b[1;32m    514\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcu_seqlens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrope_freqs_cis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrope_freqs_cis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/2070201531.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, cu_seqlens, rope_freqs_cis)\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m         attn_out = self.attention_qkvpacked(\n\u001b[0m\u001b[1;32m    471\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcu_seqlens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrope_freqs_cis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrope_freqs_cis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m         )\n","\u001b[0;32m/tmp/ipykernel_36/2070201531.py\u001b[0m in \u001b[0;36mattention_qkvpacked\u001b[0;34m(self, x, cu_seqlens, rope_freqs_cis)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0mattn_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVL_VISION_ATTENTION_FUNCTIONS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn_implementation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m         attn_out = attn_func(\n\u001b[0m\u001b[1;32m    450\u001b[0m             \u001b[0mxq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_cu_seqlens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcu_seqlens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_cu_seqlens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcu_seqlens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         )\n","\u001b[0;32m/tmp/ipykernel_36/2070201531.py\u001b[0m in \u001b[0;36msdpa_attention\u001b[0;34m(q, k, v, q_cu_seqlens, k_cu_seqlens)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaled_dot_product_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 508.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 193.12 MiB is free. Process 4361 has 15.70 GiB memory in use. Of the allocated memory 9.02 GiB is allocated by PyTorch, and 6.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 508.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 193.12 MiB is free. Process 4361 has 15.70 GiB memory in use. Of the allocated memory 9.02 GiB is allocated by PyTorch, and 6.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":30},{"cell_type":"code","source":"alsdjnfjgnsaldmsgmdsh","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T08:30:31.956022Z","iopub.status.idle":"2025-07-20T08:30:31.956366Z","shell.execute_reply.started":"2025-07-20T08:30:31.956201Z","shell.execute_reply":"2025-07-20T08:30:31.956216Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(d['img_info'].size(),d['grid_shapes'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T08:30:31.957344Z","iopub.status.idle":"2025-07-20T08:30:31.957615Z","shell.execute_reply.started":"2025-07-20T08:30:31.957498Z","shell.execute_reply":"2025-07-20T08:30:31.957511Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# out = model(d['img_info']['pixel_values'],d['img_info']['image_grid_hws'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T08:30:31.958723Z","iopub.status.idle":"2025-07-20T08:30:31.959009Z","shell.execute_reply.started":"2025-07-20T08:30:31.958847Z","shell.execute_reply":"2025-07-20T08:30:31.958861Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# torch.cat(out, dim=0).size()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T08:30:31.960348Z","iopub.status.idle":"2025-07-20T08:30:31.960672Z","shell.execute_reply.started":"2025-07-20T08:30:31.960513Z","shell.execute_reply":"2025-07-20T08:30:31.960527Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for k in d['text_info'].keys():\n print(k, d['text_info'][k].size())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T08:30:31.961683Z","iopub.status.idle":"2025-07-20T08:30:31.961881Z","shell.execute_reply.started":"2025-07-20T08:30:31.961786Z","shell.execute_reply":"2025-07-20T08:30:31.961794Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_embeds_.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T08:30:31.962383Z","iopub.status.idle":"2025-07-20T08:30:31.962597Z","shell.execute_reply.started":"2025-07-20T08:30:31.962498Z","shell.execute_reply":"2025-07-20T08:30:31.962507Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"word_embeddings = nn.Embedding(\n            128002,\n            768,\n        )\nfor i, d in enumerate(dataloader):\n    # data_inputs = torch.concatenate(d['pixel_values'], dim=0)\n    input_ids = d['text_info']['input_ids']\n    txt_emb = word_embeddings(input_ids)\n    batch_size, sequence_length, input_embed_dim = txt_emb.shape\n    data_embeds_ = model(d['img_info']['pixel_values'],d['img_info']['image_grid_hws'])\n    data_embeds_ = torch.cat(data_embeds_, dim=0).view(-1, input_embed_dim)\n    \n    \n    image_feature_nums, image_feature_dim = data_embeds_.shape\n    \n    assert image_feature_dim == input_embed_dim\n    \n    image_token_nums = (input_ids == 128001).sum().item()\n    assert image_feature_nums == image_token_nums\n    \n    # (batch_size, sequence_length, input_embed_dim) -> (batch_size * sequence_length, input_embed_dim)\n    inputs_embeds = txt_emb.reshape(-1, input_embed_dim)\n    \n    # (batch_size, sequence_length) -> (batch_size * sequence_length)\n    input_ids = input_ids.flatten()\n    \n    inputs_embeds[input_ids == 128001] = data_embeds_\n    \n    inputs_embeds = inputs_embeds.reshape(\n        (batch_size, sequence_length, input_embed_dim)\n    )\n    print(inputs_embeds.size())\n    if i>5:\n        break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T08:30:31.964346Z","iopub.status.idle":"2025-07-20T08:30:31.964638Z","shell.execute_reply.started":"2025-07-20T08:30:31.964472Z","shell.execute_reply":"2025-07-20T08:30:31.964487Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}