{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":4693932,"sourceType":"datasetVersion","datasetId":2717697},{"sourceId":247079405,"sourceType":"kernelVersion"}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"https://github.com/jahongir7174/YOLOv8-pt/tree/master","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile yolov8_model_train.py\nimport math\nimport os\nimport random\n\nimport cv2\nimport numpy\nimport torch\nfrom PIL import Image\nfrom torch.utils import data\n\nFORMATS = 'bmp', 'dng', 'jpeg', 'jpg', 'mpo', 'png', 'tif', 'tiff', 'webp'\n\n\nclass Dataset(data.Dataset):\n    def __init__(self, filenames, input_size, params, augment):\n        self.params = params\n        self.mosaic = augment\n        self.augment = augment\n        self.input_size = input_size\n\n        # Read labels\n        cache = self.load_label(filenames, self.augment)\n        labels, shapes = zip(*cache.values())\n        self.labels = list(labels)\n        self.shapes = numpy.array(shapes, dtype=numpy.float64)\n        self.filenames = list(cache.keys())  # update\n        self.n = len(shapes)  # number of samples\n        self.indices = range(self.n)\n        # Albumentations (optional, only used if package is installed)\n        self.albumentations = Albumentations()\n\n    def __getitem__(self, index):\n        index = self.indices[index]\n\n        params = self.params\n        mosaic = self.mosaic and random.random() < params['mosaic']\n\n        if mosaic:\n            shapes = None\n            # Load MOSAIC\n            image, label = self.load_mosaic(index, params)\n            # MixUp augmentation\n            if random.random() < params['mix_up']:\n                index = random.choice(self.indices)\n                mix_image1, mix_label1 = image, label\n                mix_image2, mix_label2 = self.load_mosaic(index, params)\n\n                image, label = mix_up(mix_image1, mix_label1, mix_image2, mix_label2)\n        else:\n            # Load image\n            image, shape = self.load_image(index)\n            h, w = image.shape[:2]\n\n            # Resize\n            image, ratio, pad = resize(image, self.input_size, self.augment)\n            shapes = shape, ((h / shape[0], w / shape[1]), pad)  # for COCO mAP rescaling\n\n            label = self.labels[index].copy()\n            if label.size:\n                label[:, 1:] = wh2xy(label[:, 1:], ratio[0] * w, ratio[1] * h, pad[0], pad[1])\n            if self.augment:\n                image, label = random_perspective(image, label, params)\n        nl = len(label)  # number of labels\n        if nl:\n            label[:, 1:5] = xy2wh(label[:, 1:5], image.shape[1], image.shape[0])\n\n        if self.augment:\n            # Albumentations\n            image, label = self.albumentations(image, label)\n            nl = len(label)  # update after albumentations\n            # HSV color-space\n            augment_hsv(image, params)\n            # Flip up-down\n            if random.random() < params['flip_ud']:\n                image = numpy.flipud(image)\n                if nl:\n                    label[:, 2] = 1 - label[:, 2]\n            # Flip left-right\n            if random.random() < params['flip_lr']:\n                image = numpy.fliplr(image)\n                if nl:\n                    label[:, 1] = 1 - label[:, 1]\n\n        target = torch.zeros((nl, 6))\n        if nl:\n            target[:, 1:] = torch.from_numpy(label)\n\n        # Convert HWC to CHW, BGR to RGB\n        sample = image.transpose((2, 0, 1))[::-1]\n        sample = numpy.ascontiguousarray(sample)\n\n        return torch.from_numpy(sample), target, shapes\n\n    def __len__(self):\n        return len(self.filenames)\n\n    def load_image(self, i):\n        image = cv2.imread(self.filenames[i])\n        h, w = image.shape[:2]\n        r = self.input_size / max(h, w)\n        if r != 1:\n            image = cv2.resize(image,\n                               dsize=(int(w * r), int(h * r)),\n                               interpolation=resample() if self.augment else cv2.INTER_LINEAR)\n        return image, (h, w)\n\n    def load_mosaic(self, index, params):\n        label4 = []\n        image4 = numpy.full((self.input_size * 2, self.input_size * 2, 3), 0, dtype=numpy.uint8)\n        y1a, y2a, x1a, x2a, y1b, y2b, x1b, x2b = (None, None, None, None, None, None, None, None)\n\n        border = [-self.input_size // 2, -self.input_size // 2]\n\n        xc = int(random.uniform(-border[0], 2 * self.input_size + border[1]))\n        yc = int(random.uniform(-border[0], 2 * self.input_size + border[1]))\n\n        indices = [index] + random.choices(self.indices, k=3)\n        random.shuffle(indices)\n\n        for i, index in enumerate(indices):\n            # Load image\n            image, _ = self.load_image(index)\n            shape = image.shape\n            if i == 0:  # top left\n                x1a = max(xc - shape[1], 0)\n                y1a = max(yc - shape[0], 0)\n                x2a = xc\n                y2a = yc\n                x1b = shape[1] - (x2a - x1a)\n                y1b = shape[0] - (y2a - y1a)\n                x2b = shape[1]\n                y2b = shape[0]\n            if i == 1:  # top right\n                x1a = xc\n                y1a = max(yc - shape[0], 0)\n                x2a = min(xc + shape[1], self.input_size * 2)\n                y2a = yc\n                x1b = 0\n                y1b = shape[0] - (y2a - y1a)\n                x2b = min(shape[1], x2a - x1a)\n                y2b = shape[0]\n            if i == 2:  # bottom left\n                x1a = max(xc - shape[1], 0)\n                y1a = yc\n                x2a = xc\n                y2a = min(self.input_size * 2, yc + shape[0])\n                x1b = shape[1] - (x2a - x1a)\n                y1b = 0\n                x2b = shape[1]\n                y2b = min(y2a - y1a, shape[0])\n            if i == 3:  # bottom right\n                x1a = xc\n                y1a = yc\n                x2a = min(xc + shape[1], self.input_size * 2)\n                y2a = min(self.input_size * 2, yc + shape[0])\n                x1b = 0\n                y1b = 0\n                x2b = min(shape[1], x2a - x1a)\n                y2b = min(y2a - y1a, shape[0])\n\n            image4[y1a:y2a, x1a:x2a] = image[y1b:y2b, x1b:x2b]\n            pad_w = x1a - x1b\n            pad_h = y1a - y1b\n\n            # Labels\n            label = self.labels[index].copy()\n            if len(label):\n                label[:, 1:] = wh2xy(label[:, 1:], shape[1], shape[0], pad_w, pad_h)\n            label4.append(label)\n\n        # Concat/clip labels\n        label4 = numpy.concatenate(label4, 0)\n        for x in label4[:, 1:]:\n            numpy.clip(x, 0, 2 * self.input_size, out=x)\n\n        # Augment\n        image4, label4 = random_perspective(image4, label4, params, border)\n\n        return image4, label4\n\n    @staticmethod\n    def collate_fn(batch):\n        samples, targets, shapes = zip(*batch)\n        for i, item in enumerate(targets):\n            item[:, 0] = i  # add target image index\n        return torch.stack(samples, 0), torch.cat(targets, 0), shapes\n\n    @staticmethod\n    def load_label(filenames,augment):\n        if augment:\n            path = f'/kaggle/input/vyom-ai-yolov8-v2/cocotrain2017.cache'\n        else:\n            path = f'/kaggle/input/vyom-ai-yolov8-v2/cocotval2017.cache'\n        if os.path.exists(path):\n            return torch.load(path,weights_only=False)\n        x = {}\n        for filename in filenames:\n            try:\n                # verify images\n                with open(filename, 'rb') as f:\n                    image = Image.open(f)\n                    image.verify()  # PIL verify\n                shape = image.size  # image size\n                assert (shape[0] > 9) & (shape[1] > 9), f'image size {shape} <10 pixels'\n                assert image.format.lower() in FORMATS, f'invalid image format {image.format}'\n\n                # verify labels\n                a = f'{os.sep}images{os.sep}'\n                b = f'{os.sep}labels{os.sep}'\n                if os.path.isfile(b.join(filename.rsplit(a, 1)).rsplit('.', 1)[0] + '.txt'):\n                    with open(b.join(filename.rsplit(a, 1)).rsplit('.', 1)[0] + '.txt') as f:\n                        label = [x.split() for x in f.read().strip().splitlines() if len(x)]\n                        label = numpy.array(label, dtype=numpy.float32)\n                    nl = len(label)\n                    if nl:\n                        assert label.shape[1] == 5, 'labels require 5 columns'\n                        assert (label >= 0).all(), 'negative label values'\n                        assert (label[:, 1:] <= 1).all(), 'non-normalized coordinates'\n                        _, i = numpy.unique(label, axis=0, return_index=True)\n                        if len(i) < nl:  # duplicate row check\n                            label = label[i]  # remove duplicates\n                    else:\n                        label = numpy.zeros((0, 5), dtype=numpy.float32)\n                else:\n                    label = numpy.zeros((0, 5), dtype=numpy.float32)\n                if filename:\n                    x[filename] = [label, shape]\n            except FileNotFoundError:\n                pass\n        torch.save(x, path)\n        return x\n\n\ndef wh2xy(x, w=640, h=640, pad_w=0, pad_h=0):\n    # Convert nx4 boxes\n    # from [x, y, w, h] normalized to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n    y = numpy.copy(x)\n    y[:, 0] = w * (x[:, 0] - x[:, 2] / 2) + pad_w  # top left x\n    y[:, 1] = h * (x[:, 1] - x[:, 3] / 2) + pad_h  # top left y\n    y[:, 2] = w * (x[:, 0] + x[:, 2] / 2) + pad_w  # bottom right x\n    y[:, 3] = h * (x[:, 1] + x[:, 3] / 2) + pad_h  # bottom right y\n    return y\n\n\ndef xy2wh(x, w=640, h=640):\n    # warning: inplace clip\n    x[:, [0, 2]] = x[:, [0, 2]].clip(0, w - 1E-3)  # x1, x2\n    x[:, [1, 3]] = x[:, [1, 3]].clip(0, h - 1E-3)  # y1, y2\n\n    # Convert nx4 boxes\n    # from [x1, y1, x2, y2] to [x, y, w, h] normalized where xy1=top-left, xy2=bottom-right\n    y = numpy.copy(x)\n    y[:, 0] = ((x[:, 0] + x[:, 2]) / 2) / w  # x center\n    y[:, 1] = ((x[:, 1] + x[:, 3]) / 2) / h  # y center\n    y[:, 2] = (x[:, 2] - x[:, 0]) / w  # width\n    y[:, 3] = (x[:, 3] - x[:, 1]) / h  # height\n    return y\n\n\ndef resample():\n    choices = (cv2.INTER_AREA,\n               cv2.INTER_CUBIC,\n               cv2.INTER_LINEAR,\n               cv2.INTER_NEAREST,\n               cv2.INTER_LANCZOS4)\n    return random.choice(seq=choices)\n\n\ndef augment_hsv(image, params):\n    # HSV color-space augmentation\n    h = params['hsv_h']\n    s = params['hsv_s']\n    v = params['hsv_v']\n\n    r = numpy.random.uniform(-1, 1, 3) * [h, s, v] + 1\n    h, s, v = cv2.split(cv2.cvtColor(image, cv2.COLOR_BGR2HSV))\n\n    x = numpy.arange(0, 256, dtype=r.dtype)\n    lut_h = ((x * r[0]) % 180).astype('uint8')\n    lut_s = numpy.clip(x * r[1], 0, 255).astype('uint8')\n    lut_v = numpy.clip(x * r[2], 0, 255).astype('uint8')\n\n    im_hsv = cv2.merge((cv2.LUT(h, lut_h), cv2.LUT(s, lut_s), cv2.LUT(v, lut_v)))\n    cv2.cvtColor(im_hsv, cv2.COLOR_HSV2BGR, dst=image)  # no return needed\n\n\ndef resize(image, input_size, augment):\n    # Resize and pad image while meeting stride-multiple constraints\n    shape = image.shape[:2]  # current shape [height, width]\n\n    # Scale ratio (new / old)\n    r = min(input_size / shape[0], input_size / shape[1])\n    if not augment:  # only scale down, do not scale up (for better val mAP)\n        r = min(r, 1.0)\n\n    # Compute padding\n    pad = int(round(shape[1] * r)), int(round(shape[0] * r))\n    w = (input_size - pad[0]) / 2\n    h = (input_size - pad[1]) / 2\n\n    if shape[::-1] != pad:  # resize\n        image = cv2.resize(image,\n                           dsize=pad,\n                           interpolation=resample() if augment else cv2.INTER_LINEAR)\n    top, bottom = int(round(h - 0.1)), int(round(h + 0.1))\n    left, right = int(round(w - 0.1)), int(round(w + 0.1))\n    image = cv2.copyMakeBorder(image, top, bottom, left, right, cv2.BORDER_CONSTANT)  # add border\n    return image, (r, r), (w, h)\n\n\ndef candidates(box1, box2):\n    # box1(4,n), box2(4,n)\n    w1, h1 = box1[2] - box1[0], box1[3] - box1[1]\n    w2, h2 = box2[2] - box2[0], box2[3] - box2[1]\n    aspect_ratio = numpy.maximum(w2 / (h2 + 1e-16), h2 / (w2 + 1e-16))  # aspect ratio\n    return (w2 > 2) & (h2 > 2) & (w2 * h2 / (w1 * h1 + 1e-16) > 0.1) & (aspect_ratio < 100)\n\n\ndef random_perspective(samples, targets, params, border=(0, 0)):\n    h = samples.shape[0] + border[0] * 2\n    w = samples.shape[1] + border[1] * 2\n\n    # Center\n    center = numpy.eye(3)\n    center[0, 2] = -samples.shape[1] / 2  # x translation (pixels)\n    center[1, 2] = -samples.shape[0] / 2  # y translation (pixels)\n\n    # Perspective\n    perspective = numpy.eye(3)\n\n    # Rotation and Scale\n    rotate = numpy.eye(3)\n    a = random.uniform(-params['degrees'], params['degrees'])\n    s = random.uniform(1 - params['scale'], 1 + params['scale'])\n    rotate[:2] = cv2.getRotationMatrix2D(angle=a, center=(0, 0), scale=s)\n\n    # Shear\n    shear = numpy.eye(3)\n    shear[0, 1] = math.tan(random.uniform(-params['shear'], params['shear']) * math.pi / 180)\n    shear[1, 0] = math.tan(random.uniform(-params['shear'], params['shear']) * math.pi / 180)\n\n    # Translation\n    translate = numpy.eye(3)\n    translate[0, 2] = random.uniform(0.5 - params['translate'], 0.5 + params['translate']) * w\n    translate[1, 2] = random.uniform(0.5 - params['translate'], 0.5 + params['translate']) * h\n\n    # Combined rotation matrix, order of operations (right to left) is IMPORTANT\n    matrix = translate @ shear @ rotate @ perspective @ center\n    if (border[0] != 0) or (border[1] != 0) or (matrix != numpy.eye(3)).any():  # image changed\n        samples = cv2.warpAffine(samples, matrix[:2], dsize=(w, h), borderValue=(0, 0, 0))\n\n    # Transform label coordinates\n    n = len(targets)\n    if n:\n        xy = numpy.ones((n * 4, 3))\n        xy[:, :2] = targets[:, [1, 2, 3, 4, 1, 4, 3, 2]].reshape(n * 4, 2)  # x1y1, x2y2, x1y2, x2y1\n        xy = xy @ matrix.T  # transform\n        xy = xy[:, :2].reshape(n, 8)  # perspective rescale or affine\n\n        # create new boxes\n        x = xy[:, [0, 2, 4, 6]]\n        y = xy[:, [1, 3, 5, 7]]\n        new = numpy.concatenate((x.min(1), y.min(1), x.max(1), y.max(1))).reshape(4, n).T\n\n        # clip\n        new[:, [0, 2]] = new[:, [0, 2]].clip(0, w)\n        new[:, [1, 3]] = new[:, [1, 3]].clip(0, h)\n\n        # filter candidates\n        indices = candidates(box1=targets[:, 1:5].T * s, box2=new.T)\n        targets = targets[indices]\n        targets[:, 1:5] = new[indices]\n\n    return samples, targets\n\n\ndef mix_up(image1, label1, image2, label2):\n    # Applies MixUp augmentation https://arxiv.org/pdf/1710.09412.pdf\n    alpha = numpy.random.beta(32.0, 32.0)  # mix-up ratio, alpha=beta=32.0\n    image = (image1 * alpha + image2 * (1 - alpha)).astype(numpy.uint8)\n    label = numpy.concatenate((label1, label2), 0)\n    return image, label\n\n\nclass Albumentations:\n    def __init__(self):\n        self.transform = None\n        try:\n            import albumentations as album\n\n            transforms = [album.Blur(p=0.01),\n                          album.CLAHE(p=0.01),\n                          album.ToGray(p=0.01),\n                          album.MedianBlur(p=0.01)]\n            self.transform = album.Compose(transforms,\n                                           album.BboxParams('yolo', ['class_labels']))\n\n        except ImportError:  # package not installed, skip\n            pass\n\n    def __call__(self, image, label):\n        if self.transform:\n            x = self.transform(image=image,\n                               bboxes=label[:, 1:],\n                               class_labels=label[:, 0])\n            image = x['image']\n            label = numpy.array([[c, *b] for c, b in zip(x['class_labels'], x['bboxes'])])\n        return image, label\n\nimport copy\nimport math\nimport random\nimport time\n\nimport numpy\nimport torch\nimport torchvision\nfrom torch.nn.functional import cross_entropy, one_hot\n\n\ndef setup_seed():\n    \"\"\"\n    Setup random seed.\n    \"\"\"\n    random.seed(0)\n    numpy.random.seed(0)\n    torch.manual_seed(0)\n    torch.cuda.manual_seed(0)\n    torch.backends.cudnn.deterministic = True\n\n\ndef scale(coords, shape1, shape2, ratio_pad=None):\n    if ratio_pad is None:  # calculate from img0_shape\n        gain = min(shape1[0] / shape2[0], shape1[1] / shape2[1])  # gain  = old / new\n        pad = (shape1[1] - shape2[1] * gain) / 2, (shape1[0] - shape2[0] * gain) / 2  # wh padding\n    else:\n        gain = ratio_pad[0][0]\n        pad = ratio_pad[1]\n\n    coords[:, [0, 2]] -= pad[0]  # x padding\n    coords[:, [1, 3]] -= pad[1]  # y padding\n    coords[:, :4] /= gain\n\n    coords[:, 0].clamp_(0, shape2[1])  # x1\n    coords[:, 1].clamp_(0, shape2[0])  # y1\n    coords[:, 2].clamp_(0, shape2[1])  # x2\n    coords[:, 3].clamp_(0, shape2[0])  # y2\n    return coords\n\n\ndef make_anchors(x, strides, offset=0.5):\n    \"\"\"\n    Generate anchors from features\n    \"\"\"\n    assert x is not None\n    anchor_points, stride_tensor = [], []\n    for i, stride in enumerate(strides):\n        _, _, h, w = x[i].shape\n        sx = torch.arange(end=w, dtype=x[i].dtype, device=x[i].device) + offset  # shift x\n        sy = torch.arange(end=h, dtype=x[i].dtype, device=x[i].device) + offset  # shift y\n        sy, sx = torch.meshgrid(sy, sx)\n        anchor_points.append(torch.stack((sx, sy), -1).view(-1, 2))\n        stride_tensor.append(torch.full((h * w, 1), stride, dtype=x[i].dtype, device=x[i].device))\n    return torch.cat(anchor_points), torch.cat(stride_tensor)\n\n\ndef box_iou(box1, box2):\n    # https://github.com/pytorch/vision/blob/master/torchvision/ops/boxes.py\n    \"\"\"\n    Return intersection-over-union (Jaccard index) of boxes.\n    Both sets of boxes are expected to be in (x1, y1, x2, y2) format.\n    Arguments:\n        box1 (Tensor[N, 4])\n        box2 (Tensor[M, 4])\n    Returns:\n        iou (Tensor[N, M]): the NxM matrix containing the pairwise\n            IoU values for every element in boxes1 and boxes2\n    \"\"\"\n\n    # intersection(N,M) = (rb(N,M,2) - lt(N,M,2)).clamp(0).prod(2)\n    (a1, a2), (b1, b2) = box1[:, None].chunk(2, 2), box2.chunk(2, 1)\n    intersection = (torch.min(a2, b2) - torch.max(a1, b1)).clamp(0).prod(2)\n\n    # IoU = intersection / (area1 + area2 - intersection)\n    box1 = box1.T\n    box2 = box2.T\n\n    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n\n    return intersection / (area1[:, None] + area2 - intersection)\n\n\ndef wh2xy_(x):\n    y = x.clone()\n    y[..., 0] = x[..., 0] - x[..., 2] / 2  # top left x\n    y[..., 1] = x[..., 1] - x[..., 3] / 2  # top left y\n    y[..., 2] = x[..., 0] + x[..., 2] / 2  # bottom right x\n    y[..., 3] = x[..., 1] + x[..., 3] / 2  # bottom right y\n    return y\n\n\ndef non_max_suppression(prediction, conf_threshold=0.25, iou_threshold=0.45):\n    nc = prediction.shape[1] - 4  # number of classes\n    xc = prediction[:, 4:4 + nc].amax(1) > conf_threshold  # candidates\n\n    # Settings\n    max_wh = 7680  # (pixels) maximum box width and height\n    max_det = 300  # the maximum number of boxes to keep after NMS\n    max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n\n    start = time.time()\n    outputs = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n    for index, x in enumerate(prediction):  # image index, image inference\n        # Apply constraints\n        x = x.transpose(0, -1)[xc[index]]  # confidence\n\n        # If none remain process next image\n        if not x.shape[0]:\n            continue\n\n        # Detections matrix nx6 (box, conf, cls)\n        box, cls = x.split((4, nc), 1)\n        # center_x, center_y, width, height) to (x1, y1, x2, y2)\n        box = wh2xy_(box)\n        if nc > 1:\n            i, j = (cls > conf_threshold).nonzero(as_tuple=False).T\n            x = torch.cat((box[i], x[i, 4 + j, None], j[:, None].float()), 1)\n        else:  # best class only\n            conf, j = cls.max(1, keepdim=True)\n            x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_threshold]\n        # Check shape\n        if not x.shape[0]:  # no boxes\n            continue\n        # sort by confidence and remove excess boxes\n        x = x[x[:, 4].argsort(descending=True)[:max_nms]]\n\n        # Batched NMS\n        c = x[:, 5:6] * max_wh  # classes\n        boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n        i = torchvision.ops.nms(boxes, scores, iou_threshold)  # NMS\n        i = i[:max_det]  # limit detections\n        outputs[index] = x[i]\n        if (time.time() - start) > 0.5 + 0.05 * prediction.shape[0]:\n            print(f'WARNING ⚠️ NMS time limit {0.5 + 0.05 * prediction.shape[0]:.3f}s exceeded')\n            break  # time limit exceeded\n\n    return outputs\n\n\ndef smooth(y, f=0.05):\n    # Box filter of fraction f\n    nf = round(len(y) * f * 2) // 2 + 1  # number of filter elements (must be odd)\n    p = numpy.ones(nf // 2)  # ones padding\n    yp = numpy.concatenate((p * y[0], y, p * y[-1]), 0)  # y padded\n    return numpy.convolve(yp, numpy.ones(nf) / nf, mode='valid')  # y-smoothed\n\n\ndef compute_ap(tp, conf, pred_cls, target_cls, eps=1e-16):\n    \"\"\"\n    Compute the average precision, given the recall and precision curves.\n    Source: https://github.com/rafaelpadilla/Object-Detection-Metrics.\n    # Arguments\n        tp:  True positives (nparray, nx1 or nx10).\n        conf:  Object-ness value from 0-1 (nparray).\n        pred_cls:  Predicted object classes (nparray).\n        target_cls:  True object classes (nparray).\n    # Returns\n        The average precision\n    \"\"\"\n    # Sort by object-ness\n    i = numpy.argsort(-conf)\n    tp, conf, pred_cls = tp[i], conf[i], pred_cls[i]\n\n    # Find unique classes\n    unique_classes, nt = numpy.unique(target_cls, return_counts=True)\n    nc = unique_classes.shape[0]  # number of classes, number of detections\n\n    # Create Precision-Recall curve and compute AP for each class\n    p = numpy.zeros((nc, 1000))\n    r = numpy.zeros((nc, 1000))\n    ap = numpy.zeros((nc, tp.shape[1]))\n    px, py = numpy.linspace(0, 1, 1000), []  # for plotting\n    for ci, c in enumerate(unique_classes):\n        i = pred_cls == c\n        nl = nt[ci]  # number of labels\n        no = i.sum()  # number of outputs\n        if no == 0 or nl == 0:\n            continue\n\n        # Accumulate FPs and TPs\n        fpc = (1 - tp[i]).cumsum(0)\n        tpc = tp[i].cumsum(0)\n\n        # Recall\n        recall = tpc / (nl + eps)  # recall curve\n        # negative x, xp because xp decreases\n        r[ci] = numpy.interp(-px, -conf[i], recall[:, 0], left=0)\n\n        # Precision\n        precision = tpc / (tpc + fpc)  # precision curve\n        p[ci] = numpy.interp(-px, -conf[i], precision[:, 0], left=1)  # p at pr_score\n\n        # AP from recall-precision curve\n        for j in range(tp.shape[1]):\n            m_rec = numpy.concatenate(([0.0], recall[:, j], [1.0]))\n            m_pre = numpy.concatenate(([1.0], precision[:, j], [0.0]))\n\n            # Compute the precision envelope\n            m_pre = numpy.flip(numpy.maximum.accumulate(numpy.flip(m_pre)))\n\n            # Integrate area under curve\n            x = numpy.linspace(0, 1, 101)  # 101-point interp (COCO)\n            ap[ci, j] = numpy.trapz(numpy.interp(x, m_rec, m_pre), x)  # integrate\n\n    # Compute F1 (harmonic mean of precision and recall)\n    f1 = 2 * p * r / (p + r + eps)\n\n    i = smooth(f1.mean(0), 0.1).argmax()  # max F1 index\n    p, r, f1 = p[:, i], r[:, i], f1[:, i]\n    tp = (r * nt).round()  # true positives\n    fp = (tp / (p + eps) - tp).round()  # false positives\n    ap50, ap = ap[:, 0], ap.mean(1)  # AP@0.5, AP@0.5:0.95\n    m_pre, m_rec = p.mean(), r.mean()\n    map50, mean_ap = ap50.mean(), ap.mean()\n    return tp, fp, m_pre, m_rec, map50, mean_ap\n\n\ndef strip_optimizer(filename):\n    x = torch.load(filename,weights_only=False, map_location=torch.device('cpu'))\n    x['model'].half()  # to FP16\n    for p in x['model'].parameters():\n        p.requires_grad = False\n    torch.save(x, filename)\n\n\ndef clip_gradients(model, max_norm=10.0):\n    parameters = model.parameters()\n    torch.nn.utils.clip_grad_norm_(parameters, max_norm=max_norm)\n\n\nclass EMA:\n    \"\"\"\n    Updated Exponential Moving Average (EMA) from https://github.com/rwightman/pytorch-image-models\n    Keeps a moving average of everything in the model state_dict (parameters and buffers)\n    For EMA details see https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage\n    \"\"\"\n\n    def __init__(self, model, decay=0.9999, tau=2000, updates=0):\n        # Create EMA\n        self.ema = copy.deepcopy(model).eval()  # FP32 EMA\n        self.updates = updates  # number of EMA updates\n        # decay exponential ramp (to help early epochs)\n        self.decay = lambda x: decay * (1 - math.exp(-x / tau))\n        for p in self.ema.parameters():\n            p.requires_grad_(False)\n\n    def update(self, model):\n        if hasattr(model, 'module'):\n            model = model.module\n        # Update EMA parameters\n        with torch.no_grad():\n            self.updates += 1\n            d = self.decay(self.updates)\n\n            msd = model.state_dict()  # model state_dict\n            for k, v in self.ema.state_dict().items():\n                if v.dtype.is_floating_point:\n                    v *= d\n                    v += (1 - d) * msd[k].detach().cpu()\n\n\nclass AverageMeter:\n    def __init__(self):\n        self.num = 0\n        self.sum = 0\n        self.avg = 0\n\n    def update(self, v, n):\n        if not math.isnan(float(v)):\n            self.num = self.num + n\n            self.sum = self.sum + v * n\n            self.avg = self.sum / self.num\n\n\nclass ComputeLoss:\n    def __init__(self, model, params):\n        super().__init__()\n        if hasattr(model, 'module'):\n            model = model.module\n\n        device = next(model.parameters()).device  # get model device\n\n        m = model.head  # Head() module\n        self.bce = torch.nn.BCEWithLogitsLoss(reduction='none')\n        self.stride = m.stride  # model strides\n        self.nc = m.nc  # number of classes\n        self.no = m.no\n        self.device = device\n        self.params = params\n\n        # task aligned assigner\n        self.top_k = 10\n        self.alpha = 0.5\n        self.beta = 6.0\n        self.eps = 1e-9\n\n        self.bs = 1\n        self.num_max_boxes = 0\n        # DFL Loss params\n        self.dfl_ch = m.dfl.ch\n        self.project = torch.arange(self.dfl_ch, dtype=torch.float, device=device)\n\n    def __call__(self, outputs, targets):\n        x = outputs[1] if isinstance(outputs, tuple) else outputs\n        output = torch.cat([i.view(x[0].shape[0], self.no, -1) for i in x], 2)\n        pred_output, pred_scores = output.split((4 * self.dfl_ch, self.nc), 1)\n\n        pred_output = pred_output.permute(0, 2, 1).contiguous()\n        pred_scores = pred_scores.permute(0, 2, 1).contiguous()\n\n        size = torch.tensor(x[0].shape[2:], dtype=pred_scores.dtype, device=self.device)\n        size = size * self.stride[0]\n\n        anchor_points, stride_tensor = make_anchors(x, self.stride, 0.5)\n\n        # targets\n        if targets.shape[0] == 0:\n            gt = torch.zeros(pred_scores.shape[0], 0, 5, device=self.device)\n        else:\n            i = targets[:, 0]  # image index\n            _, counts = i.unique(return_counts=True)\n            gt = torch.zeros(pred_scores.shape[0], counts.max(), 5, device=self.device)\n            for j in range(pred_scores.shape[0]):\n                matches = i == j\n                n = matches.sum()\n                if n:\n                    gt[j, :n] = targets[matches, 1:]\n            gt[..., 1:5] = wh2xy_(gt[..., 1:5].mul_(size[[1, 0, 1, 0]]))\n\n        gt_labels, gt_bboxes = gt.split((1, 4), 2)  # cls, xyxy\n        mask_gt = gt_bboxes.sum(2, keepdim=True).gt_(0)\n\n        # boxes\n        b, a, c = pred_output.shape\n        pred_bboxes = pred_output.view(b, a, 4, c // 4).softmax(3)\n        pred_bboxes = pred_bboxes.matmul(self.project.type(pred_bboxes.dtype))\n\n        a, b = torch.split(pred_bboxes, 2, -1)\n        pred_bboxes = torch.cat((anchor_points - a, anchor_points + b), -1)\n\n        scores = pred_scores.detach().sigmoid()\n        bboxes = (pred_bboxes.detach() * stride_tensor).type(gt_bboxes.dtype)\n        target_bboxes, target_scores, fg_mask = self.assign(scores, bboxes,\n                                                            gt_labels, gt_bboxes, mask_gt,\n                                                            anchor_points * stride_tensor)\n\n        target_bboxes /= stride_tensor\n        target_scores_sum = target_scores.sum()\n\n        # cls loss\n        loss_cls = self.bce(pred_scores, target_scores.to(pred_scores.dtype))\n        loss_cls = loss_cls.sum() / target_scores_sum\n\n        # box loss\n        loss_box = torch.zeros(1, device=self.device)\n        loss_dfl = torch.zeros(1, device=self.device)\n        if fg_mask.sum():\n            # IoU loss\n            weight = torch.masked_select(target_scores.sum(-1), fg_mask).unsqueeze(-1)\n            loss_box = self.iou(pred_bboxes[fg_mask], target_bboxes[fg_mask])\n            loss_box = ((1.0 - loss_box) * weight).sum() / target_scores_sum\n            # DFL loss\n            a, b = torch.split(target_bboxes, 2, -1)\n            target_lt_rb = torch.cat((anchor_points - a, b - anchor_points), -1)\n            target_lt_rb = target_lt_rb.clamp(0, self.dfl_ch - 1.01)  # distance (left_top, right_bottom)\n            loss_dfl = self.df_loss(pred_output[fg_mask].view(-1, self.dfl_ch), target_lt_rb[fg_mask])\n            loss_dfl = (loss_dfl * weight).sum() / target_scores_sum\n\n        loss_cls *= self.params['cls']\n        loss_box *= self.params['box']\n        loss_dfl *= self.params['dfl']\n        return loss_cls + loss_box + loss_dfl  # loss(cls, box, dfl)\n\n    @torch.no_grad()\n    def assign(self, pred_scores, pred_bboxes, true_labels, true_bboxes, true_mask, anchors):\n        \"\"\"\n        Task-aligned One-stage Object Detection assigner\n        \"\"\"\n        self.bs = pred_scores.size(0)\n        self.num_max_boxes = true_bboxes.size(1)\n\n        if self.num_max_boxes == 0:\n            device = true_bboxes.device\n            return (torch.full_like(pred_scores[..., 0], self.nc).to(device),\n                    torch.zeros_like(pred_bboxes).to(device),\n                    torch.zeros_like(pred_scores).to(device),\n                    torch.zeros_like(pred_scores[..., 0]).to(device),\n                    torch.zeros_like(pred_scores[..., 0]).to(device))\n\n        i = torch.zeros([2, self.bs, self.num_max_boxes], dtype=torch.long)\n        i[0] = torch.arange(end=self.bs).view(-1, 1).repeat(1, self.num_max_boxes)\n        i[1] = true_labels.long().squeeze(-1)\n\n        overlaps = self.iou(true_bboxes.unsqueeze(2), pred_bboxes.unsqueeze(1))\n        overlaps = overlaps.squeeze(3).clamp(0)\n        align_metric = pred_scores[i[0], :, i[1]].pow(self.alpha) * overlaps.pow(self.beta)\n        bs, n_boxes, _ = true_bboxes.shape\n        lt, rb = true_bboxes.view(-1, 1, 4).chunk(2, 2)  # left-top, right-bottom\n        bbox_deltas = torch.cat((anchors[None] - lt, rb - anchors[None]), dim=2)\n        mask_in_gts = bbox_deltas.view(bs, n_boxes, anchors.shape[0], -1).amin(3).gt_(1e-9)\n        metrics = align_metric * mask_in_gts\n        top_k_mask = true_mask.repeat([1, 1, self.top_k]).bool()\n        num_anchors = metrics.shape[-1]\n        top_k_metrics, top_k_indices = torch.topk(metrics, self.top_k, dim=-1, largest=True)\n        if top_k_mask is None:\n            top_k_mask = (top_k_metrics.max(-1, keepdim=True) > self.eps).tile([1, 1, self.top_k])\n        top_k_indices = torch.where(top_k_mask, top_k_indices, 0)\n        is_in_top_k = one_hot(top_k_indices, num_anchors).sum(-2)\n        # filter invalid boxes\n        is_in_top_k = torch.where(is_in_top_k > 1, 0, is_in_top_k)\n        mask_top_k = is_in_top_k.to(metrics.dtype)\n        # merge all mask to a final mask, (b, max_num_obj, h*w)\n        mask_pos = mask_top_k * mask_in_gts * true_mask\n\n        fg_mask = mask_pos.sum(-2)\n        if fg_mask.max() > 1:  # one anchor is assigned to multiple gt_bboxes\n            mask_multi_gts = (fg_mask.unsqueeze(1) > 1).repeat([1, self.num_max_boxes, 1])\n            max_overlaps_idx = overlaps.argmax(1)\n            is_max_overlaps = one_hot(max_overlaps_idx, self.num_max_boxes)\n            is_max_overlaps = is_max_overlaps.permute(0, 2, 1).to(overlaps.dtype)\n            mask_pos = torch.where(mask_multi_gts, is_max_overlaps, mask_pos)\n            fg_mask = mask_pos.sum(-2)\n        # find each grid serve which gt(index)\n        target_gt_idx = mask_pos.argmax(-2)  # (b, h*w)\n\n        # assigned target labels, (b, 1)\n        batch_index = torch.arange(end=self.bs,\n                                   dtype=torch.int64,\n                                   device=true_labels.device)[..., None]\n        target_gt_idx = target_gt_idx + batch_index * self.num_max_boxes\n        target_labels = true_labels.long().flatten()[target_gt_idx]\n\n        # assigned target boxes\n        target_bboxes = true_bboxes.view(-1, 4)[target_gt_idx]\n\n        # assigned target scores\n        target_labels.clamp(0)\n        target_scores = one_hot(target_labels, self.nc)\n        fg_scores_mask = fg_mask[:, :, None].repeat(1, 1, self.nc)\n        target_scores = torch.where(fg_scores_mask > 0, target_scores, 0)\n\n        # normalize\n        align_metric *= mask_pos\n        pos_align_metrics = align_metric.amax(axis=-1, keepdim=True)\n        pos_overlaps = (overlaps * mask_pos).amax(axis=-1, keepdim=True)\n        norm_align_metric = (align_metric * pos_overlaps / (pos_align_metrics + self.eps)).amax(-2)\n        norm_align_metric = norm_align_metric.unsqueeze(-1)\n        target_scores = target_scores * norm_align_metric\n\n        return target_bboxes, target_scores, fg_mask.bool()\n\n    @staticmethod\n    def df_loss(pred_dist, target):\n        # Return sum of left and right DFL losses\n        # Distribution Focal Loss https://ieeexplore.ieee.org/document/9792391\n        tl = target.long()  # target left\n        tr = tl + 1  # target right\n        wl = tr - target  # weight left\n        wr = 1 - wl  # weight right\n        l_loss = cross_entropy(pred_dist, tl.view(-1), reduction=\"none\").view(tl.shape)\n        r_loss = cross_entropy(pred_dist, tr.view(-1), reduction=\"none\").view(tl.shape)\n        return (l_loss * wl + r_loss * wr).mean(-1, keepdim=True)\n\n    @staticmethod\n    def iou(box1, box2, eps=1e-7):\n        # Returns Intersection over Union (IoU) of box1(1,4) to box2(n,4)\n\n        # Get the coordinates of bounding boxes\n        b1_x1, b1_y1, b1_x2, b1_y2 = box1.chunk(4, -1)\n        b2_x1, b2_y1, b2_x2, b2_y2 = box2.chunk(4, -1)\n        w1, h1 = b1_x2 - b1_x1, b1_y2 - b1_y1 + eps\n        w2, h2 = b2_x2 - b2_x1, b2_y2 - b2_y1 + eps\n\n        # Intersection area\n        area1 = b1_x2.minimum(b2_x2) - b1_x1.maximum(b2_x1)\n        area2 = b1_y2.minimum(b2_y2) - b1_y1.maximum(b2_y1)\n        intersection = area1.clamp(0) * area2.clamp(0)\n\n        # Union Area\n        union = w1 * h1 + w2 * h2 - intersection + eps\n\n        # IoU\n        iou = intersection / union\n        cw = b1_x2.maximum(b2_x2) - b1_x1.minimum(b2_x1)  # convex width\n        ch = b1_y2.maximum(b2_y2) - b1_y1.minimum(b2_y1)  # convex height\n        # Complete IoU https://arxiv.org/abs/1911.08287v1\n        c2 = cw ** 2 + ch ** 2 + eps  # convex diagonal squared\n        # center dist ** 2\n        rho2 = ((b2_x1 + b2_x2 - b1_x1 - b1_x2) ** 2 + (b2_y1 + b2_y2 - b1_y1 - b1_y2) ** 2) / 4\n        # https://github.com/Zzh-tju/DIoU-SSD-pytorch/blob/master/utils/box/box_utils.py#L47\n        v = (4 / math.pi ** 2) * (torch.atan(w2 / h2) - torch.atan(w1 / h1)).pow(2)\n        with torch.no_grad():\n            alpha = v / (v - iou + (1 + eps))\n        return iou - (rho2 / c2 + v * alpha)  # CIoU\n\nimport math\n\nimport torch\n\n# from utils.util import make_anchors\n\n\ndef pad(k, p=None, d=1):\n    if d > 1:\n        k = d * (k - 1) + 1\n    if p is None:\n        p = k // 2\n    return p\n\n\ndef fuse_conv(conv, norm):\n    fused_conv = torch.nn.Conv2d(conv.in_channels,\n                                 conv.out_channels,\n                                 kernel_size=conv.kernel_size,\n                                 stride=conv.stride,\n                                 padding=conv.padding,\n                                 groups=conv.groups,\n                                 bias=True).requires_grad_(False).to(conv.weight.device)\n\n    w_conv = conv.weight.clone().view(conv.out_channels, -1)\n    w_norm = torch.diag(norm.weight.div(torch.sqrt(norm.eps + norm.running_var)))\n    fused_conv.weight.copy_(torch.mm(w_norm, w_conv).view(fused_conv.weight.size()))\n\n    b_conv = torch.zeros(conv.weight.size(0), device=conv.weight.device) if conv.bias is None else conv.bias\n    b_norm = norm.bias - norm.weight.mul(norm.running_mean).div(torch.sqrt(norm.running_var + norm.eps))\n    fused_conv.bias.copy_(torch.mm(w_norm, b_conv.reshape(-1, 1)).reshape(-1) + b_norm)\n\n    return fused_conv\n\n\nclass Conv(torch.nn.Module):\n    def __init__(self, in_ch, out_ch, k=1, s=1, p=None, d=1, g=1):\n        super().__init__()\n        self.conv = torch.nn.Conv2d(in_ch, out_ch, k, s, pad(k, p, d), d, g, False)\n        self.norm = torch.nn.BatchNorm2d(out_ch, 0.001, 0.03)\n        self.relu = torch.nn.SiLU(inplace=True)\n\n    def forward(self, x):\n        return self.relu(self.norm(self.conv(x)))\n\n    def fuse_forward(self, x):\n        return self.relu(self.conv(x))\n\n\nclass Residual(torch.nn.Module):\n    def __init__(self, ch, add=True):\n        super().__init__()\n        self.add_m = add\n        self.res_m = torch.nn.Sequential(Conv(ch, ch, 3),\n                                         Conv(ch, ch, 3))\n\n    def forward(self, x):\n        return self.res_m(x) + x if self.add_m else self.res_m(x)\n\n\nclass CSP(torch.nn.Module):\n    def __init__(self, in_ch, out_ch, n=1, add=True):\n        super().__init__()\n        self.conv1 = Conv(in_ch, out_ch // 2)\n        self.conv2 = Conv(in_ch, out_ch // 2)\n        self.conv3 = Conv((2 + n) * out_ch // 2, out_ch)\n        self.res_m = torch.nn.ModuleList(Residual(out_ch // 2, add) for _ in range(n))\n\n    def forward(self, x):\n        y = [self.conv1(x), self.conv2(x)]\n        y.extend(m(y[-1]) for m in self.res_m)\n        return self.conv3(torch.cat(y, dim=1))\n\n\nclass SPP(torch.nn.Module):\n    def __init__(self, in_ch, out_ch, k=5):\n        super().__init__()\n        self.conv1 = Conv(in_ch, in_ch // 2)\n        self.conv2 = Conv(in_ch * 2, out_ch)\n        self.res_m = torch.nn.MaxPool2d(k, 1, k // 2)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        y1 = self.res_m(x)\n        y2 = self.res_m(y1)\n        return self.conv2(torch.cat([x, y1, y2, self.res_m(y2)], 1))\n\n\nclass DarkNet(torch.nn.Module):\n    def __init__(self, width, depth):\n        super().__init__()\n        p1 = [Conv(width[0], width[1], 3, 2)]\n        p2 = [Conv(width[1], width[2], 3, 2),\n              CSP(width[2], width[2], depth[0])]\n        p3 = [Conv(width[2], width[3], 3, 2),\n              CSP(width[3], width[3], depth[1])]\n        p4 = [Conv(width[3], width[4], 3, 2),\n              CSP(width[4], width[4], depth[2])]\n        p5 = [Conv(width[4], width[5], 3, 2),\n              CSP(width[5], width[5], depth[0]),\n              SPP(width[5], width[5])]\n\n        self.p1 = torch.nn.Sequential(*p1)\n        self.p2 = torch.nn.Sequential(*p2)\n        self.p3 = torch.nn.Sequential(*p3)\n        self.p4 = torch.nn.Sequential(*p4)\n        self.p5 = torch.nn.Sequential(*p5)\n\n    def forward(self, x):\n        p1 = self.p1(x)\n        p2 = self.p2(p1)\n        p3 = self.p3(p2)\n        p4 = self.p4(p3)\n        p5 = self.p5(p4)\n        return p3, p4, p5\n\n\nclass DarkFPN(torch.nn.Module):\n    def __init__(self, width, depth):\n        super().__init__()\n        self.up = torch.nn.Upsample(None, 2)\n        self.h1 = CSP(width[4] + width[5], width[4], depth[0], False)\n        self.h2 = CSP(width[3] + width[4], width[3], depth[0], False)\n        self.h3 = Conv(width[3], width[3], 3, 2)\n        self.h4 = CSP(width[3] + width[4], width[4], depth[0], False)\n        self.h5 = Conv(width[4], width[4], 3, 2)\n        self.h6 = CSP(width[4] + width[5], width[5], depth[0], False)\n\n    def forward(self, x):\n        p3, p4, p5 = x\n        h1 = self.h1(torch.cat([self.up(p5), p4], 1))\n        h2 = self.h2(torch.cat([self.up(h1), p3], 1))\n        h4 = self.h4(torch.cat([self.h3(h2), h1], 1))\n        h6 = self.h6(torch.cat([self.h5(h4), p5], 1))\n        return h2, h4, h6\n\n\nclass DFL(torch.nn.Module):\n    # Integral module of Distribution Focal Loss (DFL)\n    # Generalized Focal Loss https://ieeexplore.ieee.org/document/9792391\n    def __init__(self, ch=16):\n        super().__init__()\n        self.ch = ch\n        self.conv = torch.nn.Conv2d(ch, 1, 1, bias=False).requires_grad_(False)\n        x = torch.arange(ch, dtype=torch.float).view(1, ch, 1, 1)\n        self.conv.weight.data[:] = torch.nn.Parameter(x)\n\n    def forward(self, x):\n        b, c, a = x.shape\n        x = x.view(b, 4, self.ch, a).transpose(2, 1)\n        return self.conv(x.softmax(1)).view(b, 4, a)\n\n\nclass Head(torch.nn.Module):\n    anchors = torch.empty(0)\n    strides = torch.empty(0)\n\n    def __init__(self, nc=80, filters=()):\n        super().__init__()\n        self.ch = 16  # DFL channels\n        self.nc = nc  # number of classes\n        self.nl = len(filters)  # number of detection layers\n        self.no = nc + self.ch * 4  # number of outputs per anchor\n        self.stride = torch.zeros(self.nl)  # strides computed during build\n\n        c1 = max(filters[0], self.nc)\n        c2 = max((filters[0] // 4, self.ch * 4))\n\n        self.dfl = DFL(self.ch)\n        self.cls = torch.nn.ModuleList(torch.nn.Sequential(Conv(x, c1, 3),\n                                                           Conv(c1, c1, 3),\n                                                           torch.nn.Conv2d(c1, self.nc, 1)) for x in filters)\n        self.box = torch.nn.ModuleList(torch.nn.Sequential(Conv(x, c2, 3),\n                                                           Conv(c2, c2, 3),\n                                                           torch.nn.Conv2d(c2, 4 * self.ch, 1)) for x in filters)\n\n    def forward(self, x):\n        for i in range(self.nl):\n            x[i] = torch.cat((self.box[i](x[i]), self.cls[i](x[i])), 1)\n        if self.training:\n            return x\n        self.anchors, self.strides = (x.transpose(0, 1) for x in make_anchors(x, self.stride, 0.5))\n\n        x = torch.cat([i.view(x[0].shape[0], self.no, -1) for i in x], 2)\n        box, cls = x.split((self.ch * 4, self.nc), 1)\n        a, b = torch.split(self.dfl(box), 2, 1)\n        a = self.anchors.unsqueeze(0) - a\n        b = self.anchors.unsqueeze(0) + b\n        box = torch.cat(((a + b) / 2, b - a), 1)\n        return torch.cat((box * self.strides, cls.sigmoid()), 1)\n\n    def initialize_biases(self):\n        # Initialize biases\n        # WARNING: requires stride availability\n        m = self\n        for a, b, s in zip(m.box, m.cls, m.stride):\n            a[-1].bias.data[:] = 1.0  # box\n            # cls (.01 objects, 80 classes, 640 img)\n            b[-1].bias.data[:m.nc] = math.log(5 / m.nc / (640 / s) ** 2)\n\n\nclass YOLO(torch.nn.Module):\n    def __init__(self, width, depth, num_classes):\n        super().__init__()\n        self.net = DarkNet(width, depth)\n        self.fpn = DarkFPN(width, depth)\n\n        img_dummy = torch.zeros(1, 3, 256, 256)\n        self.head = Head(num_classes, (width[3], width[4], width[5]))\n        self.head.stride = torch.tensor([256 / x.shape[-2] for x in self.forward(img_dummy)])\n        self.stride = self.head.stride\n        self.head.initialize_biases()\n\n    def forward(self, x):\n        x = self.net(x)\n        x = self.fpn(x)\n        return self.head(list(x))\n\n    def fuse(self):\n        for m in self.modules():\n            if type(m) is Conv and hasattr(m, 'norm'):\n                m.conv = fuse_conv(m.conv, m.norm)\n                m.forward = m.fuse_forward\n                delattr(m, 'norm')\n        return self\n\n\ndef yolo_v8_n(num_classes: int = 80):\n    depth = [1, 2, 2]\n    width = [3, 16, 32, 64, 128, 256]\n    return YOLO(width, depth, num_classes)\n\n\ndef yolo_v8_s(num_classes: int = 80):\n    depth = [1, 2, 2]\n    width = [3, 32, 64, 128, 256, 512]\n    return YOLO(width, depth, num_classes)\n\n\ndef yolo_v8_m(num_classes: int = 80):\n    depth = [2, 4, 4]\n    width = [3, 48, 96, 192, 384, 576]\n    return YOLO(width, depth, num_classes)\n\n\ndef yolo_v8_l(num_classes: int = 80):\n    depth = [3, 6, 6]\n    width = [3, 64, 128, 256, 512, 512]\n    return YOLO(width, depth, num_classes)\n\n\ndef yolo_v8_x(num_classes: int = 80):\n    depth = [3, 6, 6]\n    width = [3, 80, 160, 320, 640, 640]\n    return YOLO(width, depth, num_classes)\n\nparams = {'lr0': 0.01,\n 'lrf': 0.01,\n 'momentum': 0.937,\n 'weight_decay': 0.0005,\n 'warmup_epochs': 3.0,\n 'warmup_momentum': 0.8,\n 'warmup_bias_lr': 0.1,\n 'box': 7.5,\n 'cls': 0.5,\n 'dfl': 1.5,\n 'hsv_h': 0.015,\n 'hsv_s': 0.7,\n 'hsv_v': 0.4,\n 'degrees': 0.0,\n 'translate': 0.1,\n 'scale': 0.5,\n 'shear': 0.0,\n 'flip_ud': 0.0,\n 'flip_lr': 0.5,\n 'mosaic': 1.0,\n 'mix_up': 0.0,\n 'names': {0: 'person',\n  1: 'bicycle',\n  2: 'car',\n  3: 'motorcycle',\n  4: 'airplane',\n  5: 'bus',\n  6: 'train',\n  7: 'truck',\n  8: 'boat',\n  9: 'traffic light',\n  10: 'fire hydrant',\n  11: 'stop sign',\n  12: 'parking meter',\n  13: 'bench',\n  14: 'bird',\n  15: 'cat',\n  16: 'dog',\n  17: 'horse',\n  18: 'sheep',\n  19: 'cow',\n  20: 'elephant',\n  21: 'bear',\n  22: 'zebra',\n  23: 'giraffe',\n  24: 'backpack',\n  25: 'umbrella',\n  26: 'handbag',\n  27: 'tie',\n  28: 'suitcase',\n  29: 'frisbee',\n  30: 'skis',\n  31: 'snowboard',\n  32: 'sports ball',\n  33: 'kite',\n  34: 'baseball bat',\n  35: 'baseball glove',\n  36: 'skateboard',\n  37: 'surfboard',\n  38: 'tennis racket',\n  39: 'bottle',\n  40: 'wine glass',\n  41: 'cup',\n  42: 'fork',\n  43: 'knife',\n  44: 'spoon',\n  45: 'bowl',\n  46: 'banana',\n  47: 'apple',\n  48: 'sandwich',\n  49: 'orange',\n  50: 'broccoli',\n  51: 'carrot',\n  52: 'hot dog',\n  53: 'pizza',\n  54: 'donut',\n  55: 'cake',\n  56: 'chair',\n  57: 'couch',\n  58: 'potted plant',\n  59: 'bed',\n  60: 'dining table',\n  61: 'toilet',\n  62: 'tv',\n  63: 'laptop',\n  64: 'mouse',\n  65: 'remote',\n  66: 'keyboard',\n  67: 'cell phone',\n  68: 'microwave',\n  69: 'oven',\n  70: 'toaster',\n  71: 'sink',\n  72: 'refrigerator',\n  73: 'book',\n  74: 'clock',\n  75: 'vase',\n  76: 'scissors',\n  77: 'teddy bear',\n  78: 'hair drier',\n  79: 'toothbrush'}}\n\nfrom accelerate import Accelerator, DistributedDataParallelKwargs\nimport argparse\nimport copy\nimport csv\nimport os\nimport warnings\n\nimport numpy\nimport torch\nimport tqdm\nimport yaml\nfrom torch.utils import data\nwarnings.filterwarnings(\"ignore\")\n\n\ndef learning_rate(args, params):\n    def fn(x):\n        return (1 - x / args.epochs) * (1.0 - params['lrf']) + params['lrf']\n\n    return fn\ndef train(args, params):\n\n    # Model\n    model = yolo_v8_s(len(params['names'].values()))\n   \n    # model.to(device)\n\n    # Optimizer\n    accumulate = max(round(64 / (args.batch_size*2)), 1)\n    ddp_kwargs = DistributedDataParallelKwargs() #find_unused_parameters=True\n    accelerator = Accelerator(mixed_precision=\"fp16\",gradient_accumulation_steps=accumulate,kwargs_handlers=[ddp_kwargs])#\"bf16\"\n    device = accelerator.device\n    ema = EMA(model)\n    params['weight_decay'] *= args.batch_size * accelerator.num_processes * accumulate / 64\n\n    p = [], [], []\n    for v in model.modules():\n        if hasattr(v, 'bias') and isinstance(v.bias, torch.nn.Parameter):\n            p[2].append(v.bias)\n        if isinstance(v, torch.nn.BatchNorm2d):\n            p[1].append(v.weight)\n        elif hasattr(v, 'weight') and isinstance(v.weight, torch.nn.Parameter):\n            p[0].append(v.weight)\n\n    optimizer = torch.optim.SGD(p[2], params['lr0'], params['momentum'], nesterov=True)\n    optimizer.add_param_group({'params': p[0], 'weight_decay': params['weight_decay']})\n    optimizer.add_param_group({'params': p[1]})\n    del p\n\n    # Scheduler\n    lr = learning_rate(args, params)\n    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr)\n\n    # EMA\n   \n    if accelerator.is_main_process:\n        print(f\"Using device: {device}\")\n        print(f\"Number of processes: {accelerator.num_processes}\")\n        print(f\"Distributed type: {accelerator.distributed_type}\")\n\n    filenames = []\n    with open('../input/ultralytics/datasets/coco/train2017.txt') as reader:\n        for filename in reader.readlines():\n            filename = filename.rstrip().split('/')[-1]\n            filenames.append('../input/ultralytics/datasets/coco/images/train2017/' + filename)\n\n    dataset = Dataset(filenames, args.input_size, params, True)\n    loader = data.DataLoader(dataset, batch_size=args.batch_size,\n                             num_workers=2, pin_memory=True, collate_fn=Dataset.collate_fn)\n\n    filenames = []\n    with open('../input/ultralytics/datasets/coco/val2017.txt') as reader:\n        for filename in reader.readlines():\n            filename = filename.rstrip().split('/')[-1]\n            filenames.append('../input/ultralytics/datasets/coco/images/val2017/' + filename)\n\n    test_dataset = Dataset(filenames, args.input_size, params, False)\n    test_loader = data.DataLoader(test_dataset, batch_size=8, shuffle=False,\n                             num_workers=2, pin_memory=True, collate_fn=Dataset.collate_fn)\n\n    # Prepare with Accelerator\n    if args.distributed:\n        # DDP mode\n        model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)\n    model,optimizer, loader,test_loader, scheduler = accelerator.prepare(model, optimizer, loader,test_loader, scheduler)\n\n    best = 0\n    num_batch = len(loader)\n    criterion = ComputeLoss(model, params)\n    num_warmup = max(round(params['warmup_epochs'] * num_batch), 1000)\n\n    if accelerator.is_main_process:\n        f = open('step.csv', 'w')\n        writer = csv.DictWriter(f, fieldnames=['epoch', 'mAP@50', 'mAP'])\n        writer.writeheader()\n\n    for epoch in range(args.epochs):\n        model.train()\n        epoch_loss = 0.0\n        if args.epochs - epoch == 10:\n            loader.dataset.mosaic = False\n\n        m_loss = AverageMeter()\n        p_bar = tqdm.tqdm(enumerate(loader), total=num_batch, disable=not accelerator.is_main_process)\n        optimizer.zero_grad()\n\n        for i, (samples, targets, _) in p_bar:\n            x = i + num_batch * epoch\n            samples = samples.float() / 255\n            targets = targets\n\n            # Warmup\n            if x <= num_warmup:\n                xp = [0, num_warmup]\n                fp = [1, 64 / args.batch_size]\n                accumulate = max(1, numpy.interp(x, xp, fp).round())\n                for j, y in enumerate(optimizer.param_groups):\n                    if j == 0:\n                        fp = [params['warmup_bias_lr'], y['initial_lr'] * lr(epoch)]\n                    else:\n                        fp = [0.0, y['initial_lr'] * lr(epoch)]\n                    y['lr'] = numpy.interp(x, xp, fp)\n                    if 'momentum' in y:\n                        fp = [params['warmup_momentum'], params['momentum']]\n                        y['momentum'] = numpy.interp(x, xp, fp)\n\n            with accelerator.accumulate(model):\n                outputs = model(samples)\n                loss = criterion(outputs, targets)\n                accelerator.backward(loss)\n                accelerator.clip_grad_norm_(model.parameters(), 1.0)\n                optimizer.step()\n                optimizer.zero_grad()\n                \n                if ema:\n                    ema.update(model)\n\n            m_loss.update(loss.item(), samples.size(0))\n            \n            \n\n            if accelerator.is_main_process:\n                memory = f'{torch.cuda.memory_reserved() / 1E9:.3g}G'\n                s = ('%10s' * 2 + '%10.4g') % (f'{epoch + 1}/{args.epochs}', memory, m_loss.avg)\n                p_bar.set_description(s)\n            loss_value = loss.detach().float().item()\n            epoch_loss += loss_value\n\n        scheduler.step()\n        gathered_epoch_loss = accelerator.gather(torch.tensor(epoch_loss, device=device)).sum().item()\n        avg_loss = gathered_epoch_loss / (len(loader) * accelerator.num_processes)\n            \n        if accelerator.is_main_process:\n            temp =  ema.ema #accelerator.unwrap_model(model) #ema.ema\n            temp_1 = accelerator.unwrap_model(model)\n            # temp = copy.deepcopy(ema.ema)\n            torch.save(temp.state_dict(), 'ema_last.pt')\n            torch.save(temp_1.state_dict(), 'last.pt')\n            \n            accelerator.print(f'train-epoch-{epoch+1} loss-{avg_loss}')\n            # last = test(args, params,test_loader,accelerator, model) #ema.ema\n            # writer.writerow({'mAP': str(f'{last[1]:.3f}'),\n            #                  'epoch': str(epoch + 1).zfill(3),\n            #                  'mAP@50': str(f'{last[0]:.3f}')})\n            # f.flush()\n            # temp = accelerator.unwrap_model(model) #ema.ema\n            # # temp = copy.deepcopy(ema.ema)\n            # torch.save(temp.state_dict(), 'last.pt')\n    \n    \n            # if last[1] > best:\n            #     best = last[1]\n                \n            # if best == last[1]:\n            #     torch.save(temp.state_dict(), 'best.pt')\n            del temp\n\n    if accelerator.is_main_process:\n        f.close()\n        # strip_optimizer('best.pt')\n        # strip_optimizer('last.pt')\n        \n    if accelerator.is_main_process:\n        accelerator.end_training()\n        print(\"Training completed!\")\n\n\n@torch.no_grad()\ndef test(args, params,test_loader,accelerator, model):\n   \n    device = accelerator.device\n    model.eval()\n    # model = model.half()\n\n    iou_v = torch.linspace(0.5, 0.95, 10).to(device)\n    n_iou = iou_v.numel()\n\n    m_pre = m_rec = map50 = mean_ap = 0.\n    metrics = []\n\n    p_bar = tqdm.tqdm(test_loader, desc=('%10s' * 3) % ('precision', 'recall', 'mAP'), disable=not accelerator.is_main_process)\n    for samples, targets, shapes in p_bar:\n        samples = samples.half() / 255\n        targets = targets\n\n        _, _, height, width = samples.shape\n        with torch.no_grad():\n            outputs = model(samples)\n        targets,outputs = accelerator.gather_for_metrics((targets,outputs))\n\n        # NMS\n        targets[:, 2:] *= torch.tensor((width, height, width, height), device=device)\n        outputs = non_max_suppression(outputs, 0.001, 0.65)\n\n        # Metrics\n        for i, output in enumerate(outputs):\n            labels = targets[targets[:, 0] == i, 1:]\n            correct = torch.zeros(output.shape[0], n_iou, dtype=torch.bool, device=device)\n\n            if output.shape[0] == 0:\n                if labels.shape[0]:\n                    metrics.append((correct, *torch.zeros((3, 0), device=device)))\n                continue\n\n            detections = output.clone()\n            scale(detections[:, :4], samples[i].shape[1:], shapes[i][0], shapes[i][1])\n\n            if labels.shape[0]:\n                tbox = labels[:, 1:5].clone()\n                tbox[:, 0] = labels[:, 1] - labels[:, 3] / 2\n                tbox[:, 1] = labels[:, 2] - labels[:, 4] / 2\n                tbox[:, 2] = labels[:, 1] + labels[:, 3] / 2\n                tbox[:, 3] = labels[:, 2] + labels[:, 4] / 2\n                scale(tbox, samples[i].shape[1:], shapes[i][0], shapes[i][1])\n\n                correct_np = numpy.zeros((detections.shape[0], iou_v.shape[0]), dtype=bool)\n\n                t_tensor = torch.cat((labels[:, 0:1], tbox), 1)\n                iou = box_iou(t_tensor[:, 1:], detections[:, :4])\n                correct_class = t_tensor[:, 0:1] == detections[:, 5]\n\n                for j in range(len(iou_v)):\n                    x = torch.where((iou >= iou_v[j]) & correct_class)\n                    if x[0].shape[0]:\n                        matches = torch.cat((torch.stack(x, 1), iou[x[0], x[1]][:, None]), 1)\n                        matches = matches.cpu().numpy()\n                        if x[0].shape[0] > 1:\n                            matches = matches[matches[:, 2].argsort()[::-1]]\n                            matches = matches[numpy.unique(matches[:, 1], return_index=True)[1]]\n                            matches = matches[numpy.unique(matches[:, 0], return_index=True)[1]]\n                        correct_np[matches[:, 1].astype(int), j] = True\n\n                correct = torch.tensor(correct_np, dtype=torch.bool, device=device)\n\n            metrics.append((correct, output[:, 4], output[:, 5], labels[:, 0]))\n\n    # Compute metrics\n    if len(metrics):\n        metrics = [torch.cat(x, 0).cpu().numpy() for x in zip(*metrics)]\n        if metrics[0].any():\n            tp, fp, m_pre, m_rec, map50, mean_ap = compute_ap(*metrics)\n\n    if accelerator.is_main_process:\n        accelerator.print('%10.3g' * 3 % (m_pre, m_rec, mean_ap))\n        \n\n    # model.float()\n    return map50, mean_ap\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--input-size', default=640, type=int)\n    parser.add_argument('--batch-size', default=32, type=int)\n    parser.add_argument('--local_rank', default=0, type=int)\n    parser.add_argument('--epochs', default=18, type=int)\n\n    args = parser.parse_args('')\n\n    args.local_rank = int(os.getenv('LOCAL_RANK', 0))\n    args.world_size = int(os.getenv('WORLD_SIZE', 1))\n    args.distributed = int(os.getenv('WORLD_SIZE', 1)) > 1\n\n    setup_seed()\n    \n    train(args, params)\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T09:43:34.953807Z","iopub.execute_input":"2025-06-28T09:43:34.954453Z","iopub.status.idle":"2025-06-28T09:43:34.994615Z","shell.execute_reply.started":"2025-06-28T09:43:34.954421Z","shell.execute_reply":"2025-06-28T09:43:34.993846Z"}},"outputs":[{"name":"stdout","text":"Writing yolov8_model_train.py\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"! accelerate launch --num_processes=2 ../working/yolov8_model_train.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T09:43:34.995983Z","iopub.execute_input":"2025-06-28T09:43:34.996299Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda:0\nNumber of processes: 2\nDistributed type: DistributedType.MULTI_GPU\n      1/18     10.3G     13.63:   4%|▎        | 71/1849 [02:13<48:14,  1.63s/it]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}