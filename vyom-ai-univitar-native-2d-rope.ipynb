{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8588492,"sourceType":"datasetVersion","datasetId":5137030}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install -q -U flash-attn --no-index --find-links ../input/flash-attn/flash_attn-2.5.9.post1-cp310-cp310-linux_x86_64.whl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T19:06:37.596792Z","iopub.execute_input":"2025-07-09T19:06:37.597541Z","iopub.status.idle":"2025-07-09T19:06:37.602669Z","shell.execute_reply.started":"2025-07-09T19:06:37.597508Z","shell.execute_reply":"2025-07-09T19:06:37.601362Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# from flash_attn.bert_padding import pad_input\n# from flash_attn.flash_attn_interface import flash_attn_varlen_qkvpacked_func as flash_attn_unpadded_qkvpacked_func","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T19:06:37.604671Z","iopub.execute_input":"2025-07-09T19:06:37.604988Z","iopub.status.idle":"2025-07-09T19:06:37.625370Z","shell.execute_reply.started":"2025-07-09T19:06:37.604963Z","shell.execute_reply":"2025-07-09T19:06:37.623920Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"from typing import Iterable, Optional, Tuple, Union, List \n\nimport os\nimport math\nimport json\nimport torch\nimport numpy as np\nimport torch.nn as nn\nimport torch.utils.checkpoint\nimport torch.nn.functional as F\n\nfrom PIL import Image\nfrom einops import rearrange\nfrom functools import partial\nfrom timm.layers import DropPath\nfrom dataclasses import dataclass\nfrom torchvision import transforms\nfrom transformers.utils import logging\nfrom transformers.activations import ACT2FN\n# from transformers.modeling_utils import PreTrainedModel\nfrom transformers.configuration_utils import PretrainedConfig\nfrom transformers.modeling_outputs import BaseModelOutput, ModelOutput\n# from flash_attn.bert_padding import pad_input\n# from flash_attn.flash_attn_interface import flash_attn_varlen_qkvpacked_func as flash_attn_unpadded_qkvpacked_func\n\nlogger = logging.get_logger(__name__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T19:06:37.627969Z","iopub.execute_input":"2025-07-09T19:06:37.628509Z","iopub.status.idle":"2025-07-09T19:06:37.647617Z","shell.execute_reply.started":"2025-07-09T19:06:37.628467Z","shell.execute_reply":"2025-07-09T19:06:37.645980Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def rotate_half(x):\n    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n    x1 = x[..., : x.shape[-1] // 2]\n    x2 = x[..., x.shape[-1] // 2 :]\n    return torch.cat((-x2, x1), dim=-1)\n\n\ndef apply_rotary_pos_emb_vision(tensor: torch.Tensor, freqs: torch.Tensor) -> torch.Tensor:\n    orig_dtype = tensor.dtype\n    tensor = tensor.float()\n    cos = freqs.cos()\n    sin = freqs.sin()\n    cos = cos.unsqueeze(1).repeat(1, 1, 2).unsqueeze(0).float()\n    sin = sin.unsqueeze(1).repeat(1, 1, 2).unsqueeze(0).float()\n    output = (tensor * cos) + (rotate_half(tensor) * sin)\n    output = output.to(orig_dtype)\n    return output\n\n\nclass VisionRotaryEmbedding2D(nn.Module):\n    def __init__(self, dim: int, theta: float = 10000.0) -> None:\n        super().__init__()\n        inv_freq = 1.0 / (theta ** (torch.arange(0, dim, 2, dtype=torch.float) / dim))\n        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n\n    def forward_(self, seqlen: int) -> torch.Tensor:\n        seq = torch.arange(seqlen, device=self.inv_freq.device, dtype=self.inv_freq.dtype)\n        freqs = torch.outer(seq, self.inv_freq)\n        return freqs\n    \n    def forward(self, grid_shapes, spatial_merge_size=2):\n        pos_ids = []\n        s = spatial_merge_size\n        for t, h, w in grid_shapes:\n            hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)\n            hpos_ids = hpos_ids.reshape(h // s, s, w // s, s)\n            hpos_ids = hpos_ids.permute(0, 2, 1, 3)\n            hpos_ids = hpos_ids.flatten()\n            wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)\n            wpos_ids = wpos_ids.reshape(h // s, s, w // s, s)\n            wpos_ids = wpos_ids.permute(0, 2, 1, 3)\n            wpos_ids = wpos_ids.flatten()\n            pos_ids.append(torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))\n        pos_ids = torch.cat(pos_ids, dim=0)\n        max_grid_size = torch.tensor(grid_shapes).max()\n        rotary_pos_emb_full = self.forward_(max_grid_size)\n        rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)\n        return rotary_pos_emb\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\n\nclass NativeFlashAttention(nn.Module):\n    \"\"\"Native PyTorch equivalent of FlashAttention.\n\n    Arguments\n    ---------\n    softmax_scale : Optional[float]\n        The temperature to use for softmax. If None, defaults to 1/sqrt(d).\n    attention_dropout : float\n        Dropout probability on attention weights.\n    \"\"\"\n\n    def __init__(self, softmax_scale=None, attention_dropout=0.0, device=None, dtype=None):\n        super().__init__()\n        self.softmax_scale = softmax_scale\n        self.dropout = nn.Dropout(attention_dropout)\n\n    def forward(self, qkv, key_padding_mask=None, causal=False, cu_seqlens=None,\n                max_s=None, need_weights=False):\n        \"\"\"\n        Arguments\n        ---------\n        qkv : Tensor\n            Shape (B, S, 3, H, D). Query, Key, Value combined.\n        key_padding_mask : Optional[Tensor]\n            Shape (B, S), where True indicates padding.\n        causal : bool\n            Whether to apply causal masking.\n        \"\"\"\n        assert not need_weights, \"Returning attention weights is not supported in this native version.\"\n        assert qkv.dim() == 5, f\"Expected qkv shape (B, S, 3, H, D), got {qkv.shape}\"\n\n        B, S, _, H, D = qkv.shape\n\n        # Split qkv\n        q, k, v = qkv.unbind(dim=2)  # Each is (B, S, H, D)\n\n        # Compute attention scores\n        scale = self.softmax_scale or 1.0 / math.sqrt(D)\n        attn_scores = torch.einsum('bshd,bt hd->bhst', q, k) * scale  # (B, H, S, S)\n\n        # Causal mask\n        if causal:\n            mask = torch.tril(torch.ones(S, S, device=q.device, dtype=torch.bool))\n            attn_scores = attn_scores.masked_fill(~mask, float('-inf'))\n\n        # Key padding mask\n        if key_padding_mask is not None:\n            # Expand mask to match attn_scores: (B, 1, 1, S)\n            kp_mask = key_padding_mask[:, None, None, :].to(torch.bool)  # (B, 1, 1, S)\n            attn_scores = attn_scores.masked_fill(kp_mask, float('-inf'))\n\n        # Softmax\n        attn_probs = F.softmax(attn_scores, dim=-1)\n        attn_probs = self.dropout(attn_probs)\n\n        # Compute attention output\n        attn_output = torch.einsum('bhst,bthd->bshd', attn_probs, v)  # (B, S, H, D)\n\n        return attn_output, None\n\n\nimport torch\nimport torch.nn.functional as F\nfrom typing import Optional\n\nimport torch\nimport torch.nn.functional as F\nfrom typing import Optional\n\ndef pytorch_native_flash_attn_unpadded_qkvpacked(\n    qkv: torch.Tensor,\n    cu_seqlens: torch.Tensor,\n    max_s: int,\n    dropout_p: float = 0.0,\n    softmax_scale: Optional[float] = None,\n    causal: bool = False,\n) -> torch.Tensor:\n    \"\"\"\n    A PyTorch native implementation that directly mirrors the call format of \n    flash_attn_unpadded_qkvpacked_func.\n\n    Args:\n        qkv: A tensor of shape (total_tokens, 3, num_heads, head_dim).\n        cu_seqlens: Cumulative sequence lengths, shape (batch_size + 1,).\n        max_s: The maximum sequence length in the batch.\n        dropout_p: Dropout probability.\n        softmax_scale: Scaling factor for softmax. Defaults to 1/sqrt(head_dim).\n        causal: If True, applies a causal mask for autoregressive decoding.\n\n    Returns:\n        The output tensor from the attention operation, with shape \n        (total_tokens, num_heads, head_dim).\n    \"\"\"\n    total_tokens, _, num_heads, head_dim = qkv.shape\n    batch_size = len(cu_seqlens) - 1\n\n    # Unpack Q, K, and V from the single input tensor\n    q, k, v = qkv.unbind(dim=1)\n\n    # Create padded tensors from the unpadded (\"varlen\") input\n    q_padded = torch.zeros((batch_size, max_s, num_heads, head_dim), dtype=q.dtype, device=q.device)\n    k_padded = torch.zeros((batch_size, max_s, num_heads, head_dim), dtype=k.dtype, device=k.device)\n    v_padded = torch.zeros((batch_size, max_s, num_heads, head_dim), dtype=v.dtype, device=v.device)\n\n    # Fill the padded tensors using the cumulative sequence lengths\n    for i in range(batch_size):\n        start, end = cu_seqlens[i], cu_seqlens[i + 1]\n        seq_len = end - start\n        q_padded[i, :seq_len] = q[start:end]\n        k_padded[i, :seq_len] = k[start:end]\n        v_padded[i, :seq_len] = v[start:end]\n\n    # Create the attention mask to ignore padding\n    # The mask should be True for positions we want to ignore\n    attn_mask = torch.arange(max_s, device=q.device)[None, :] >= (cu_seqlens[1:] - cu_seqlens[:-1])[:, None]\n    \n    # If causal, combine the padding mask with a causal mask\n    if causal:\n        # `is_causal=True` in scaled_dot_product_attention is the most efficient way\n        # It handles both causal masking and the padding mask internally.\n        pass\n    elif attn_mask.any():\n        # The mask needs to be expanded for the attention heads\n        # Shape: (batch_size, num_heads, max_s, max_s)\n        # We need to reshape the mask to (batch_size, 1, 1, max_s) for broadcasting\n        attn_mask = attn_mask.reshape(batch_size, 1, 1, max_s)\n    else:\n        attn_mask = None # No mask needed if there's no padding\n\n    # Permute to the B, H, T, D format expected by the backend\n    q_padded = q_padded.permute(0, 2, 1, 3)\n    k_padded = k_padded.permute(0, 2, 1, 3)\n    v_padded = v_padded.permute(0, 2, 1, 3)\n\n    # Use the optimized backend for scaled dot-product attention\n    # For causal masking with padding, the `attn_mask` correctly informs the function\n    # about the valid sequence lengths.\n    output_padded = F.scaled_dot_product_attention(\n        q_padded, k_padded, v_padded,\n        attn_mask=attn_mask,\n        dropout_p=dropout_p,\n        is_causal=causal # This is more efficient than manual masking\n    )\n\n    # Permute back to B, T, H, D and then unpad the output\n    output_padded = output_padded.permute(0, 2, 1, 3)\n    \n    # Concatenate the valid sequence lengths from the padded output tensor\n    output_unpadded = torch.cat(\n        [output_padded[i, :cu_seqlens[i+1] - cu_seqlens[i]] for i in range(batch_size)],\n        dim=0\n    )\n\n    return output_unpadded\nclass FlashAttention(nn.Module):\n    # https://github.com/Dao-AILab/flash-attention/blob/v0.2.8/flash_attn/flash_attention.py\n    \"\"\"Implement the scaled dot product attention with softmax.\n    Arguments\n    ---------\n        softmax_scale: The temperature to use for the softmax attention.\n                      (default: 1/sqrt(d_keys) where d_keys is computed at\n                      runtime)\n        attention_dropout: The dropout rate to apply to the attention\n                           (default: 0.0)\n    \"\"\"\n\n    def __init__(self, softmax_scale=None, attention_dropout=0.0, device=None, dtype=None):\n        super().__init__()\n        self.softmax_scale = softmax_scale\n        self.dropout_p = attention_dropout\n        self._deterministic = True\n\n    def forward(self, qkv, key_padding_mask=None, causal=False, cu_seqlens=None,\n                max_s=None, need_weights=False):\n        \"\"\"Implements the multihead softmax attention.\n        Arguments\n        ---------\n            qkv: The tensor containing the query, key, and value. (B, S, 3, H, D) if key_padding_mask is None\n                if unpadded: (nnz, 3, h, d)\n            key_padding_mask: a bool tensor of shape (B, S)\n        \"\"\"\n        # assert not need_weights\n        # assert qkv.dtype in [torch.float16, torch.bfloat16]\n        # assert qkv.is_cuda\n\n        if cu_seqlens is None:\n            batch_size = qkv.shape[0]\n            seqlen = qkv.shape[1]\n            if key_padding_mask is None:\n                qkv = rearrange(qkv, 'b s ... -> (b s) ...')\n                max_s = seqlen\n                cu_seqlens = torch.arange(0, (batch_size + 1) * seqlen, step=seqlen, dtype=torch.int32,\n                                          device=qkv.device)\n                output = pytorch_native_flash_attn_unpadded_qkvpacked(\n                    qkv, cu_seqlens, max_s, self.dropout_p if self.training else 0.0,\n                    softmax_scale=self.softmax_scale, causal=causal\n                )\n                output = rearrange(output, '(b s) ... -> b s ...', b=batch_size)\n            else:\n                qkv = qkv.squeeze()  # [1, n, h, d] -> [n, h, d]\n                seqlens_in_batch = key_padding_mask.sum(dim=-1, dtype=torch.int32)\n                max_seqlen_in_batch = seqlens_in_batch.max().item()\n                cu_seqlens = F.pad(torch.cumsum(seqlens_in_batch, dim=0, dtype=torch.torch.int32), (1, 0))\n                output = pytorch_native_flash_attn_unpadded_qkvpacked(\n                    qkv, cu_seqlens, max_seqlen_in_batch, self.dropout_p if self.training else 0.0,\n                    softmax_scale=self.softmax_scale, causal=causal\n                )\n                output = output.unsqueeze(0)\n        else:\n            assert max_s is not None\n            output = flash_attn_unpadded_qkvpacked_func(\n                qkv, cu_seqlens, max_s, self.dropout_p if self.training else 0.0,\n                softmax_scale=self.softmax_scale, causal=causal\n            )\n\n        return output, None\n\n\nclass RMSNorm(nn.Module):\n    def __init__(self, hidden_size, eps=1e-6):\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(hidden_size))\n        self.variance_epsilon = eps\n\n    def forward(self, hidden_states):\n        input_dtype = hidden_states.dtype\n        hidden_states = hidden_states.to(torch.float32)\n        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * hidden_states.to(input_dtype)\n\n\ntry:\n    from apex.normalization import FusedRMSNorm\n    RMSNorm = FusedRMSNorm  # noqa\n    logger.info('Discovered apex.normalization.FusedRMSNorm - will use it instead of RMSNorm')\nexcept ImportError:  # using the normal RMSNorm\n    pass\nexcept Exception:\n    logger.warning('discovered apex but it failed to load, falling back to RMSNorm')\n    pass\n\n\n@dataclass\nclass BaseModelOutputWithKwargs(ModelOutput):\n    last_hidden_state: torch.FloatTensor = None\n    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n    kwargs: Optional[dict] = None\n\n\nclass UniViTARVisionConfig(PretrainedConfig):\n    def __init__(\n            self,\n            resolution_mode=\"native\",\n            init_method=\"xavier\",\n            num_channels=3,\n            patch_size=16,\n            temporal_patch_size=2,\n            image_size=1792,\n            patch_dropout=0.0,\n            attention_dropout=0.0,\n            dropout=0.0,\n            drop_path_rate=0.0,\n            initializer_range=1e-10,\n            num_hidden_layers=24,\n            num_attention_heads=16,\n            hidden_size=1024,\n            intermediate_size=4224,\n            patch_embedding_bias=True,\n            qk_normalization=True,\n            qkv_bias=False,\n            initializer_factor=0.1,\n            use_pre_norm=False,\n            pe_type=\"rope2d\",\n            rope_theta=10000,\n            spatial_merge_size=1,\n            norm_type=\"RMSNorm\",\n            hidden_act='SwiGLU',\n            use_flash_attn=True,\n            layer_norm_eps=1e-6,\n            min_tokens=576,\n            max_tokens=16384,\n            image_mean=(0.485, 0.456, 0.406),\n            image_std=(0.229, 0.224, 0.225),\n            relarge_ratio=1.0,\n            **kwargs,\n    ):\n        super().__init__(**kwargs)\n\n        self.resolution_mode = resolution_mode\n        self.init_method = init_method\n        self.pe_type = pe_type\n        self.rope_theta = rope_theta\n        self.temporal_patch_size = temporal_patch_size\n        self.resize_factor = 2\n        self.num_channels = num_channels\n        self.patch_size = patch_size\n        self.image_size = image_size\n        self.patch_dropout = patch_dropout\n        self.attention_dropout = attention_dropout\n        self.dropout = dropout\n        self.drop_path_rate = drop_path_rate\n        self.initializer_range = initializer_range\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads \n        self.hidden_size = hidden_size\n        self.intermediate_size = intermediate_size\n        self.patch_embedding_bias = patch_embedding_bias\n        self.qk_normalization = qk_normalization\n        self.qkv_bias = qkv_bias\n        self.initializer_factor = initializer_factor\n        self.use_pre_norm = use_pre_norm\n        self.norm_type = norm_type\n        self.hidden_act = hidden_act\n        self.use_flash_attn = use_flash_attn\n        self.layer_norm_eps = layer_norm_eps\n        self.spatial_merge_size = spatial_merge_size\n        self.min_tokens = min_tokens\n        self.max_tokens = max_tokens\n        self.image_mean = image_mean\n        self.image_std = image_std\n        self.relarge_ratio = relarge_ratio\n\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> 'PretrainedConfig':\n        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n\n        if 'vision_config' in config_dict:\n            config_dict = config_dict['vision_config']\n\n        if 'model_type' in config_dict and hasattr(cls, 'model_type') and config_dict['model_type'] != cls.model_type:\n            logger.warning(\n                f\"You are using a model of type {config_dict['model_type']} to instantiate a model of type \"\n                f'{cls.model_type}. This is not supported for all configurations of models and can yield errors.'\n            )\n\n        return cls.from_dict(config_dict, **kwargs)\n\n\nclass UniViTARImageTransform(object):\n    def __init__(self, config):\n        self.config = config\n        self.resolution_mode = config.resolution_mode\n        \n        self.image_mean, self.image_std = config.image_mean, config.image_std\n        self.patch_size = config.patch_size\n        self.temporal_patch_size = config.temporal_patch_size\n        self.spatial_merge_size = config.spatial_merge_size\n        self.resize_factor = config.patch_size * config.spatial_merge_size * config.resize_factor\n        self.relarge_ratio = config.relarge_ratio\n\n        self.forced_transform = None\n        self.min_pixels, self.max_pixels = None, None\n        assert self.resolution_mode in [\"native\", \"224\", \"378\", \"756\"]\n        if self.resolution_mode == \"native\":\n            self.min_pixels = config.min_tokens * config.patch_size * config.patch_size\n            self.max_pixels = config.max_tokens * config.patch_size * config.patch_size\n        else:\n            image_size = int(self.resolution_mode)\n            self.forced_transform = transforms.Compose([\n                    transforms.Resize((image_size, image_size), interpolation=transforms.InterpolationMode.BICUBIC),\n                    self.convert_to_rgb,\n                    transforms.ToTensor(),\n                    transforms.Normalize(mean=self.image_mean, std=self.image_std)\n                ]\n            )\n\n    def __call__(self, images):\n        \n        if not isinstance(images, List):\n            images = [images]  # shape of each image is [h, w, c]\n        assert len(images) == 1 or len(images) % self.temporal_patch_size == 0\n\n        if self.resolution_mode == \"native\":\n            sample_num = 1 if len(images) == 1 else len(images) // self.temporal_patch_size\n            min_pixels, max_pixels = self.min_pixels // sample_num, self.max_pixels // sample_num\n            width, height = images[0].size  # (w, h)\n            if self.relarge_ratio > 0 and self.relarge_ratio != 1:\n                height, width = int(height * self.relarge_ratio), int(width * self.relarge_ratio)\n            resized_height, resized_width = self.smart_resize(height, width, self.resize_factor, min_pixels, max_pixels)\n            processed_images = []\n            for image in images:\n                image = self.convert_to_rgb(image)\n                image = self.resize(image, size=(resized_height, resized_width), resample=Image.Resampling.BICUBIC)\n                image = self.rescale(image, scale=1/255)\n                image = self.normalize(image=image, mean=self.image_mean, std=self.image_std)\n                processed_images.append(image)\n            processed_images = np.array(processed_images)  # (num, h, w, c)\n            processed_images = processed_images.transpose(0, 3, 1, 2)  # (num, c, h, w)\n        else:\n            processed_images = [self.forced_transform(image).numpy() for image in images]\n            processed_images = np.array(processed_images)\n\n        if processed_images.shape[0] == 1:\n            processed_images = np.tile(processed_images, (self.temporal_patch_size, 1, 1, 1))\n\n        return torch.from_numpy(processed_images)    \n    \n    @staticmethod\n    def convert_to_rgb(image):\n        if not isinstance(image, Image.Image):\n            return image\n        # `image.convert(\"RGB\")` would only work for .jpg images, as it creates a wrong background\n        # for transparent images. The call to `alpha_composite` handles this case\n        if image.mode == \"RGB\":\n            return image\n        image_rgba = image.convert(\"RGBA\")\n        background = Image.new(\"RGBA\", image_rgba.size, (255, 255, 255))\n        alpha_composite = Image.alpha_composite(background, image_rgba)\n        alpha_composite = alpha_composite.convert(\"RGB\")\n        return alpha_composite\n    \n    @staticmethod\n    def resize(image, size, resample, return_numpy: bool = True) -> np.ndarray:\n        \"\"\"\n        Resizes `image` to `(height, width)` specified by `size` using the PIL library.\n        \"\"\"\n        if not len(size) == 2:\n            raise ValueError(\"size must have 2 elements\")\n        assert isinstance(image, Image.Image)\n        height, width = size\n        resample = resample if resample is not None else Image.Resampling.BILINEAR\n        # PIL images are in the format (width, height)\n        resized_image = image.resize((width, height), resample=resample, reducing_gap=None)\n        if return_numpy:\n            resized_image = np.array(resized_image)\n            resized_image = np.expand_dims(resized_image, axis=-1) if resized_image.ndim == 2 else resized_image\n        return resized_image\n\n    @staticmethod\n    def rescale(image: np.ndarray, scale: float, dtype: np.dtype = np.float32) -> np.ndarray:\n        if not isinstance(image, np.ndarray):\n            raise TypeError(f\"Input image must be of type np.ndarray, got {type(image)}\")\n        rescaled_image = image * scale\n        rescaled_image = rescaled_image.astype(dtype)\n        return rescaled_image\n\n    @staticmethod\n    def normalize(image, mean, std) -> np.ndarray:\n        \"\"\"\n        Normalizes `image` using the mean and standard deviation specified by `mean` and `std`.\n        image = (image - mean) / std\n        \"\"\"\n        if not isinstance(image, np.ndarray):\n            raise ValueError(\"image must be a numpy array\")\n        num_channels = image.shape[-1]\n        # We cast to float32 to avoid errors that can occur when subtracting uint8 values.\n        # We preserve the original dtype if it is a float type to prevent upcasting float16.\n        if not np.issubdtype(image.dtype, np.floating):\n            image = image.astype(np.float32)\n        if isinstance(mean, Iterable):\n            if len(mean) != num_channels:\n                raise ValueError(f\"mean must have {num_channels} elements if it is an iterable, got {len(mean)}\")\n        else:\n            mean = [mean] * num_channels\n        mean = np.array(mean, dtype=image.dtype)\n        if isinstance(std, Iterable):\n            if len(std) != num_channels:\n                raise ValueError(f\"std must have {num_channels} elements if it is an iterable, got {len(std)}\")\n        else:\n            std = [std] * num_channels\n        std = np.array(std, dtype=image.dtype)\n        image = (image - mean) / std\n        return image\n    \n    @staticmethod\n    def smart_resize(height, width, factor, min_pixels, max_pixels):\n        \"\"\" \n        1. Both dimensions (height and width) are divisible by 'factor'.\n        2. The total number of pixels is within the range ['min_pixels', 'max_pixels'].\n        3. The aspect ratio of the image is maintained as closely as possible.\n        \"\"\"\n        if height < factor or width < factor:\n            if height < factor:\n                ratio = factor / height\n                height, width = factor, int(ratio * width) + 1\n            if width < factor:\n                ratio = factor / width\n                width, height = factor, int(ratio * height) + 1\n        h_bar = round(height / factor) * factor\n        w_bar = round(width / factor) * factor\n        if h_bar * w_bar > max_pixels:\n            beta = math.sqrt((height * width) / max_pixels)\n            h_bar = math.floor(height / beta / factor) * factor\n            w_bar = math.floor(width / beta / factor) * factor\n        elif h_bar * w_bar < min_pixels:\n            beta = math.sqrt(min_pixels / (height * width))\n            h_bar = math.ceil(height * beta / factor) * factor\n            w_bar = math.ceil(width * beta / factor) * factor\n        return h_bar, w_bar\n\n\nclass SwiGLU(nn.Module):\n    def __init__(self, config: UniViTARVisionConfig):\n        super().__init__()\n        self.config = config\n        self.inner_hidden_size = int(config.intermediate_size * 2 / 3)\n        self.act = ACT2FN['silu']\n        self.fc1 = nn.Linear(config.hidden_size, self.inner_hidden_size)\n        self.fc2 = nn.Linear(self.inner_hidden_size, config.hidden_size)\n        self.fc3 = nn.Linear(config.hidden_size, self.inner_hidden_size)\n        self.norm = RMSNorm(self.inner_hidden_size, eps=config.layer_norm_eps)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        hidden_states = self.fc1(x)\n        hidden_states = self.act(hidden_states)\n        hidden_states = self.fc2(self.norm(hidden_states * self.fc3(x)))\n        return hidden_states\n\n\nclass Attention(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n    def __init__(self, config: UniViTARVisionConfig):\n        super().__init__()\n        self.config = config\n        self.embed_dim = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.embed_dim // self.num_heads\n        assert config.use_flash_attn is True,  \"FlashAttention must be used!\"\n        assert self.head_dim * self.num_heads == self.embed_dim\n        \n        self.qkv = nn.Linear(self.embed_dim, 3 * self.embed_dim, bias=config.qkv_bias)\n        self.inner_attn = FlashAttention(attention_dropout=config.attention_dropout)\n        self.proj = nn.Linear(self.embed_dim, self.embed_dim)\n        self.proj_drop = nn.Dropout(config.dropout)\n        if self.config.qk_normalization:\n            self.q_norm = RMSNorm(self.embed_dim, eps=config.layer_norm_eps)\n            self.k_norm = RMSNorm(self.embed_dim, eps=config.layer_norm_eps) \n\n    def forward(self, hidden_states: torch.Tensor, **kwargs) -> torch.Tensor: \n        key_padding_mask = kwargs.get(\"key_padding_mask\", None)\n        rotary_pos_emb = kwargs[\"rotary_pos_emb\"]\n\n        qkv = self.qkv(hidden_states)\n        qkv = rearrange(qkv, '... (three h d) -> ... three h d', three=3, h=self.num_heads)\n        bind_dim = qkv.dim() - 3\n        target_dtype = qkv.dtype\n        q, k, v = qkv.unbind(bind_dim)\n        q = apply_rotary_pos_emb_vision(q.unsqueeze(0), rotary_pos_emb).squeeze(0)\n        k = apply_rotary_pos_emb_vision(k.unsqueeze(0), rotary_pos_emb).squeeze(0)\n        if self.config.qk_normalization:\n            q = self.q_norm(q.flatten(-2, -1)).view(q.shape)\n            k = self.k_norm(k.flatten(-2, -1)).view(k.shape)\n        qkv = torch.stack([q, k, v], dim=bind_dim).to(target_dtype)\n        context, _ = self.inner_attn(qkv, key_padding_mask=key_padding_mask, causal=False)\n        \n        outs = self.proj(rearrange(context, '... h d -> ... (h d)')) # input expected to be: [b s h d] or [s h d]\n        outs = self.proj_drop(outs)\n\n        return outs\n\n\nclass UniViTARVisionEmbeddings(nn.Module):\n    def __init__(self, config: UniViTARVisionConfig):\n        super().__init__()\n        self.config = config\n        self.embed_dim = config.hidden_size\n        self.patch_size = config.patch_size\n        self.temporal_patch_size = config.temporal_patch_size\n        self.kernel_size = [self.temporal_patch_size, self.patch_size, self.patch_size]\n        self.use_bias = config.patch_embedding_bias\n        self.patch_embedding = nn.Conv3d(\n            in_channels=3, out_channels=self.embed_dim, kernel_size=self.kernel_size, stride=self.kernel_size, bias=self.use_bias)\n\n    def forward(self, pixel_values: torch.FloatTensor, **kwargs) -> torch.Tensor:  \n        pixel_values = pixel_values.view(-1, 3, *self.kernel_size)\n        patch_embeds = self.patch_embedding(pixel_values)\n        embeddings = patch_embeds.view(1, -1, self.embed_dim)\n        self.num_patches = embeddings.shape[1]\n        return embeddings\n\n\nclass UniViTARVisionEncoderLayer(nn.Module):\n    def __init__(self, config: UniViTARVisionConfig, drop_path_rate: float):\n        super().__init__()\n        self.embed_dim = config.hidden_size\n        assert config.hidden_act == \"SwiGLU\"\n\n        self.attn = Attention(config)\n        self.norm1 = RMSNorm(self.embed_dim, eps=config.layer_norm_eps)\n        self.norm2 = RMSNorm(self.embed_dim, eps=config.layer_norm_eps)\n        self.mlp = SwiGLU(config)\n        \n        self.ls1 = nn.Parameter(config.initializer_factor * torch.ones(self.embed_dim))\n        self.ls2 = nn.Parameter(config.initializer_factor * torch.ones(self.embed_dim))\n        self.drop_path1 = DropPath(drop_path_rate) if drop_path_rate > 0. else nn.Identity()\n        self.drop_path2 = DropPath(drop_path_rate) if drop_path_rate > 0. else nn.Identity()\n\n    def forward(self, hidden_states: torch.Tensor, **kwargs):\n        hidden_states = hidden_states + self.drop_path1(self.attn(self.norm1(hidden_states), **kwargs) * self.ls1)\n        hidden_states = hidden_states + self.drop_path2(self.mlp(self.norm2(hidden_states)) * self.ls2)\n        return hidden_states\n\n\nclass UniViTARVisionEncoder(nn.Module):\n    \"\"\" Transformer encoder consisting of `config.num_hidden_layers` self attention layers. \"\"\"\n    def __init__(self, config: UniViTARVisionConfig):\n        super().__init__()\n        self.config = config\n        self.gradient_checkpointing = True\n\n        # stochastic depth decay rule\n        dpr = [x.item() for x in torch.linspace(0, config.drop_path_rate, config.num_hidden_layers)]\n        self.layers = nn.ModuleList([UniViTARVisionEncoderLayer(config, dpr[idx]) for idx in range(config.num_hidden_layers)])\n        if self.config.pe_type == \"rope2d\":\n            head_dim = config.hidden_size // config.num_attention_heads\n            self.rotary_pos_emb = VisionRotaryEmbedding2D(head_dim // 2, theta=self.config.rope_theta)\n        else:\n            raise NotImplementedError\n\n    def forward(self, inputs_embeds, output_hidden_states = False, **kwargs):\n        kwargs[\"rotary_pos_emb\"] = self.rotary_pos_emb(kwargs[\"grid_shapes\"], self.config.spatial_merge_size)\n        \n        encoder_states = () if output_hidden_states else None\n        hidden_states = inputs_embeds\n        for idx, encoder_layer in enumerate(self.layers):\n            if output_hidden_states:\n                encoder_states = encoder_states + (hidden_states,)\n            if self.gradient_checkpointing and self.training:\n                encoder_layer_forward = partial(encoder_layer, **kwargs)\n                layer_outputs = torch.utils.checkpoint.checkpoint(encoder_layer_forward, hidden_states, use_reentrant=True)\n            else:\n                layer_outputs = encoder_layer(hidden_states, **kwargs)\n            hidden_states = layer_outputs\n        if output_hidden_states:\n            encoder_states = encoder_states + (hidden_states,)\n\n        return BaseModelOutputWithKwargs(last_hidden_state=hidden_states, hidden_states=encoder_states, kwargs=kwargs)\n\n\nclass UniViTARVisionModel(nn.Module):\n    main_input_name = 'pixel_values'\n    config_class = UniViTARVisionConfig\n    _no_split_modules = ['UniViTARVisionEncoderLayer']\n\n    def __init__(self, *args, **kwargs):\n\n        config = UniViTARVisionConfig()\n\n        super().__init__()\n        self.config = config\n        self.image_transform = UniViTARImageTransform(config)\n\n        self.embeddings = UniViTARVisionEmbeddings(config)\n        self.encoder = UniViTARVisionEncoder(config)\n\n    def get_input_embeddings(self):\n        return self.embeddings\n        \n    def get_padding_mask(self, grid_shapes):\n        seq_len = torch.tensor([int((np.prod(thw) - 1) + 1) for thw in grid_shapes])\n        max_len = torch.max(seq_len)\n        batch_size = len(grid_shapes)\n        mask = torch.zeros((batch_size, max_len), dtype=torch.long)\n        range_matrix = torch.arange(max_len).expand(batch_size, max_len)\n        mask = (range_matrix < seq_len.unsqueeze(1))\n        return mask\n\n    def forward(self, pixel_values, output_hidden_states = False, **kwargs):\n        assert len(pixel_values.shape) == 2, \"(batch_num_tokens, hidden_size)\"\n        assert \"grid_shapes\" in kwargs, \"grid_shapes: [(t, h, w), ..., (t, h, w)]\"\n        kwargs[\"key_padding_mask\"] = self.get_padding_mask(kwargs[\"grid_shapes\"])\n        hidden_states = self.embeddings(pixel_values, **kwargs)\n        encoder_outputs = self.encoder(hidden_states, output_hidden_states, **kwargs)\n        last_hidden_state = encoder_outputs.last_hidden_state\n        return last_hidden_state.squeeze(0)\n    \n    def data_patchify(self, input_data):\n        t, c, h, w = input_data.shape\n        grid_t, grid_h, grid_w = t // self.config.temporal_patch_size, h // self.config.patch_size, w // self.config.patch_size\n        grid_size = c * self.config.temporal_patch_size * self.config.patch_size * self.config.patch_size\n        input_data = input_data.reshape(\n            grid_t, self.config.temporal_patch_size, c, \n            grid_h // self.config.spatial_merge_size, self.config.spatial_merge_size, self.config.patch_size, \n            grid_w // self.config.spatial_merge_size, self.config.spatial_merge_size, self.config.patch_size\n        )\n        input_data = input_data.permute(0, 3, 6, 4, 7, 2, 1, 5, 8)\n        input_data = input_data.reshape(grid_t * grid_h * grid_w, grid_size).contiguous()\n        grid_shape = (grid_t, grid_h, grid_w)\n        return input_data, grid_shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T19:07:37.135995Z","iopub.execute_input":"2025-07-09T19:07:37.136375Z","iopub.status.idle":"2025-07-09T19:07:37.238108Z","shell.execute_reply.started":"2025-07-09T19:07:37.136350Z","shell.execute_reply":"2025-07-09T19:07:37.236958Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"model = UniViTARVisionModel()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T19:07:37.385282Z","iopub.execute_input":"2025-07-09T19:07:37.385624Z","iopub.status.idle":"2025-07-09T19:07:40.479261Z","shell.execute_reply.started":"2025-07-09T19:07:37.385603Z","shell.execute_reply":"2025-07-09T19:07:40.478121Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"to_pil = transforms.ToPILImage()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T19:07:40.480943Z","iopub.execute_input":"2025-07-09T19:07:40.481345Z","iopub.status.idle":"2025-07-09T19:07:40.486821Z","shell.execute_reply.started":"2025-07-09T19:07:40.481318Z","shell.execute_reply":"2025-07-09T19:07:40.485598Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"images = [to_pil(torch.rand(3, 256, 384)),to_pil(torch.rand(3, 256, 256))]\ndata_inputs_, grid_shapes = [], []\nfor image in images:\n    data_item = model.image_transform(image)\n    input_data, grid_shape = model.data_patchify(data_item)\n    data_inputs_.append(input_data)\n    grid_shapes.append(grid_shape)\ndata_inputs = torch.concatenate(data_inputs_, dim=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T19:16:41.598705Z","iopub.execute_input":"2025-07-09T19:16:41.599091Z","iopub.status.idle":"2025-07-09T19:16:41.644150Z","shell.execute_reply.started":"2025-07-09T19:16:41.599065Z","shell.execute_reply":"2025-07-09T19:16:41.643183Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"data_inputs.size()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T19:16:43.869861Z","iopub.execute_input":"2025-07-09T19:16:43.870193Z","iopub.status.idle":"2025-07-09T19:16:43.877205Z","shell.execute_reply.started":"2025-07-09T19:16:43.870170Z","shell.execute_reply":"2025-07-09T19:16:43.876102Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"torch.Size([1176, 1536])"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"for k in data_inputs_:\n    print(k.size())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T19:16:48.911502Z","iopub.execute_input":"2025-07-09T19:16:48.911896Z","iopub.status.idle":"2025-07-09T19:16:48.918473Z","shell.execute_reply.started":"2025-07-09T19:16:48.911871Z","shell.execute_reply":"2025-07-09T19:16:48.917368Z"}},"outputs":[{"name":"stdout","text":"torch.Size([600, 1536])\ntorch.Size([576, 1536])\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"# Forward: (N1+...+Nn, P) --> [(N1, D), ..., (Nn, D)]\ndata_embeds = model(pixel_values=data_inputs, grid_shapes=grid_shapes)\ndata_embeds = data_embeds.split([np.prod(grid_shape) for grid_shape in grid_shapes])\nprint(data_embeds[0].shape, data_embeds[1].shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T19:07:42.922181Z","iopub.execute_input":"2025-07-09T19:07:42.923207Z","iopub.status.idle":"2025-07-09T19:07:52.394806Z","shell.execute_reply.started":"2025-07-09T19:07:42.923177Z","shell.execute_reply":"2025-07-09T19:07:52.393605Z"}},"outputs":[{"name":"stdout","text":"torch.Size([600, 1024]) torch.Size([576, 1024])\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}